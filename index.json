[
    {
        "content": "21th Sept 2024 - What to choose - Academia or Corporate? 23rd Jan 2024 - Old IIT Bombay Pictures 1st Dec 2023 - Nostalgic Games 7th July 2023 - Dreams 11 December 2022 - My AI/ML track in IIT Bombay 18 April 2022 - My favourite Anime Characters from every anime I have watched 21 April 2021 - Let's make a MNIST classifier over 99.57% accuracy using Pytorch 20 Sept 2020 - Crazy Creepy Theories 22nd July 2020 - Practical Machine Learning 22nd December 2019 - Why I prefer Linux over Windows O.S. (opinionated view) 22nd December 2019 - Working with IBM Watson 9th October 2019 - Visualizations generated by JJCluster (Jimut-Jisnoo Cluster) algorithm 19th September 2019 - Playing a class of games using CNN 11th August 2019 - JBP Scripts - My PDF Writings 28th July 2019 - A note on Binary Relations 13th June 2019 - My Games Link 4th June 2019 - All Pairs Algorithm using C++ (Archived) June 2017 - Codepen Stuffs",
        "permalink": "https://jimut123.github.io/blog.html",
        "title": "BLOGS",
        "summary": "21th Sept 2024 - What to choose - Academia or Corporate? 23rd Jan 2024 - Old IIT Bombay Pictures 1st"
    },
    {
        "content": "Download [ Resume : ] [ Old CV : ] 2021 Ramkrishna Mission Vivekananda Educational and Research Institute(RKMVERI) M.Sc. Computer Science 2019 - 2021 Summary The thing that I like the most about this institution is its flexibility in changing to any kind of curriculum when the student’s need arises. Here I have been nurtured by some of the state-of-the art research topics in diverse field which is not available in any university in our state. Have been supervised by some of the best faculties in India, and learned a lot from them. The rigorous training phase of my life will bear fruit sometime in the future. 2019 St. Xavier's College B.Sc. Computer Science (Hons.) 2016 - 2019 Summary Always wanted to study here, and finally dream came true after getting 95.75% in ISC. I found out that getting marks in not relevant anymore and there are many folks who are equally good as me. Did much of development and extracurricular stuffs related to software , like developed Xavotsav website , attended blood donation camps , been event head at some competitions also winning in some , and attended several workshops on the way. Made some really great friends Bodhi and Nilayan who did his job quite well in hardware (3D printer) and stuffs . Also conducted certain events like Google MLCC along with coordinating others (National Service Scheme [NSS]) , and even been head of Graphics design team . Learnt that knowledge is everything, and we need to do other things apart from just studies. This is the time where I realized how good I am in extracurricular activities and learning online by doing real world projects and co-authoring some papers. 2016 Central Modern School ICSE and ISC 2008 - 2016 Summary Happy to be the part of this institution where I learnt to do extracurricular activities like collecting funds for blind person’s association , global cancer concern (India) , been the class monitor at class 8 (developed leadership qualities on the way), also participated in sports and constantly won 3 legged race (gold and silvers) , felicitated from Baranagar municipality two times for getting good score in class 10 and 12 , ranked in class 11 and finally left the institution with good and sweet memories . 2008 St. Agnes Branch School, IIT KGP Primary School, till class 5. 2002 - 2008 Summary One of the most cherishable days in my life in IIT Kharagpur Campus , mainly focused in everything apart from studies. Made good friends, loved sports and building things with mud. Also, did swimming and occasionally participated in school sports ( from class 1 and class 4 ).",
        "permalink": "https://jimut123.github.io/about.html",
        "title": "ABOUT",
        "summary": "Download [ Resume : ] [ Old CV : ] 2021 Ramkrishna Mission Vivekananda Educational and Research Inst"
    },
    {
        "content": "E-mail Primary card title Research Gate Primary card title Google Scholar Primary card title Github Primary card title YouTube Primary card title LinkedIn Primary card title ORCiD Primary card title arXiv Primary card title Hackerrank Primary card title Twitter Primary card title Instagram Primary card title Facebook Primary card title",
        "permalink": "https://jimut123.github.io/contact.html",
        "title": "CONTACT",
        "summary": "E-mail Primary card title Research Gate Primary card title Google Scholar Primary card title Github "
    },
    {
        "content": "Mar '24 - Our paper on QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation Challenge is now out on Arxiv. Mar '24 - My M.Sc. thesis (outdated, not so well written, images super compressed) is now openly available . Please look into the paper for getting the idea. Mar '24 - We have made the RV-PBS dataset openly available. Mar '24 - Our paper titled Advancing instance segmentation and WBC classification in peripheral blood smear through domain adaptation - A study on PBC and the novel RV-PBS datasets based on my M.Sc. thesis got accepted in Expert Systems With Applications journal (IF: 8.5, Q1). Dec '23 - I will be offering Applications of Computer Vision and Deep Learning course at RKMVERI (my alma mater). Oct '23 - Awarded the prestigious Prime Minister's Research Fellowship as a Lateral Fellow Cycle 11. Grateful and humbled. Jan '23 - Our paper on Improving Multi Scale Attention Networks - Bayesian Optimization for Segmenting medical images accepted in The Imaging Science Journal (IF: 1.175, Q2, Acceptance rate: 8%) . Dec '22 - New paper out. June '22 - Grateful to get the C-MInDS fellowship for getting the highest entrance test and interview scores . June '22 - Excited to start my Ph.D. at IIT Bombay . May '22 - Selected for Ph.D. at IIT Gandhinagar , IIT Kanpur and IIT Bombay . Jan '22 - Paper on Holistic Networks accepted at BrainLes MICCAI Workshop . Oct '21 - Our paper on Chest X-Ray COVID-19 dataset accepted at EE-RDS Conference 2021 . ( With Plenary Speaker: Yoshua Bengio ). July '21 - Successfully defended my Master's Thesis titled Instance Segmentation of Peripheral Blood Smear and Refining Classification via Domain Adaptation . Resource person present in committee: Suryaprakash Kompalli . Consecutively Getting the highest marks for the thesis.",
        "permalink": "https://jimut123.github.io/news_archive.html",
        "title": "News Archives",
        "summary": "Mar '24 - Our paper on QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation Challenge"
    },
    {
        "content": "404 error !! Sorry! this page isn't available!",
        "permalink": "https://jimut123.github.io/404.html",
        "title": "404 error !!",
        "summary": "404 error !! Sorry! this page isn't available!"
    },
    {
        "content": "Symmetric Jimut Bahan Pal This was an evening project. A Symmetric Gibberish Artwork Creator to show that even Gibberish things when symmetric can look beautiful! jimutmap The Jimutmap Contributors [ ] [ ] [ ] [ ] [ show ] API to get enormous amount of high resolution satellite images from apple maps quickly through multi-threading! Deceiving computers in Reverse Turing Test through Deep Learning Jimut Bahan Pal [ arXiv ] [ slide ] [ ] Project about breaking CAPTCHAs with Deep Learning Wisp - A preference based location finder application Jimut Bahan Pal [ blog ] [ arXiv ] [ slide ] [ ] [ ] [ ] [ ] A preference based location finder app for final year project! Want to find almost any place according to your preference ? in real time? then this app is for you. It uses Python3, tkinter for GUI and Folium Maps for visualisation! Xavotsav site 2018 Jimut Bahan Pal, Bikash Pandey, Souptik Mazumder, Kalyan Mazumder, Arion Mitra and Akash Manna [ ] [ show ] [ PLAY ONLINE ] College's fest website! Graphics Design - Analytica Jimut Bahan Pal, Subhajit Karmakar, Saikat Ganguly, Lagnajit Pradhan, Ayon Tarafdar and Bodhiswatta Saha [ ] Graphics work done in Math Dept. Fest - Analytica (St. Xavier's College, Kolkata) MIS Application using tkinter Jimut Bahan Pal [ ] [ ] [ ] Tkinter application. JGD- Jimut's Git Downloader Jimut Bahan Pal [ ] [ ] [ ] CLI tool to download folders/ repo with ease, without any token! jimner - Jimut's banner CLI Jimut Bahan Pal [ ] [ ] [ ] A banner prototype for CLI/ UNIX / LINUX /WINDOWS. ASCII art! ROLLER MADNESS GAME. Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] Game built in Unity3D. Box Shooter Game Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] Game built in Unity3D. Super Sparty Brothers 2D Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] Mario like game. Solar System Simulation Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] Playing with unity 3D simulations! FPS GAME beta Jimut Bahan Pal [ ] [ ] FPS game built in Unity3D! Classic Pong Game Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] A game developed in Codeskulptor. Classic Memory Game Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] A game developed in Codeskulptor. A Stopwatch Game Jimut Bahan Pal [ ] [ ] [ PLAY ONLINE ] A game developed in Codeskulptor. Spammify Jimut Bahan Pal [ ] [ ] To send spam emails with ease! Scrawll Jimut Bahan Pal [ ] [ ] [ ] To download HD wallpapers with ease Pager Rank and Web Crawler Jimut Bahan Pal [ ] [ ] [ show ] A crawler build in Python3 which ranks pages according to pager rank algorithm Portfolio Page Jimut Bahan Pal [ ] My personal webpage framework build from scratch using Jekyll, Python3, HTML, CSS, and JS. Contains web page generators build using Ruby.",
        "permalink": "https://jimut123.github.io/projects.html",
        "title": "PROJECTS",
        "summary": "Symmetric Jimut Bahan Pal This was an evening project. A Symmetric Gibberish Artwork Creator to show"
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/gallery.html",
        "title": "GALLERY",
        "summary": ""
    },
    {
        "content": "Event Head (ENIGMA - Coding event) . Science association . Organisation - St. Xavier's College. Date - January 2019. ,valid till present. St. Xavier’s College Science Association. SIGMA - 2019.. Project Intern . datasutram . Organisation - Datasutram. Date - Dec - Jan 2019 (for 2 months) ,valid till present. Key-note Speaker . (Google) . Organisation - St. Xavier's College. Date - November 2018 ,valid till present. Machine Learning Crash Course, St. Xavier's College. Head of Graphics design team . Organisation - St. Xavier's College. Date - 2018 ,valid till present. Analytica, Maths dept. fest,. Developed Xavotsav Website - 2k18 . Organisation - St. Xavier's College. Date - 2018 ,valid till present. Volunteer . Organisation - St. Xavier's College. Date - 2017 ,valid till present. Exabyte, Computer dept. fest. Yellow Belt . Organisation - Karate-do. Volunteer in Social Work . Organisation - Global Cancer Concern India. Date - 2011 ,valid till present. Social Work, Central Modern School. Blood Donor . State Blood Transfusion Council . Organisation - St. Xavier's College, Kolkata. Date - 2017 ,valid till present. NSS. Volunteer in Social Work . Organisation - Blind Person's Association. Date - 2010 ,valid till present. Social Work, Central Modern School, Kolkata.",
        "permalink": "https://jimut123.github.io/experience.html",
        "title": "EXPERIENCES",
        "summary": "Event Head (ENIGMA - Coding event) . Science association . Organisation - St. Xavier's College. Date"
    },
    {
        "content": "Miscellaneous International Conference on Image Processing (ICIP) - Abu Dhabi Trip (Oct 2024) MICCAI Trip - Morocco and Quatar Trip (Oct-2024) Departmental trip to Alibag and Varsoli beach (March, 2023) Sanjay Gandhi National Park & Kanheri caves (Dec, 2022) Marine Drive Trip (Sept, 2022)",
        "permalink": "https://jimut123.github.io/random.html",
        "title": "RANDOM",
        "summary": "Miscellaneous International Conference on Image Processing (ICIP) - Abu Dhabi Trip (Oct 2024) MICCAI"
    },
    {
        "content": "Advancing instance segmentation and WBC classification in peripheral blood smear through domain adaptation - A study on PBC and the novel RV-PBS datasets Jimut Bahan Pal , Aniket Bhattacharyea, Debasis Banerjee and Tamal Maharaj Expert Systems With Applications. ( 2024 ). Find the Novel RV-PBS dataset here . Abstract Article Slide Code Bibtex Automating blood cell counting and detection from smear slides holds significant potential for aiding doctors in disease diagnosis through blood tests. However, existing literature has not adequately addressed using whole slide data in this context. This study introduces the novel RV-PBS dataset, comprising ten distinct peripheral blood smear classes, each featuring multiple multi-class White Blood Cells per slide, specifically designed, for instance segmentation benchmarks. While conventional instance segmentation models like Mask R-CNN exhibit promising results in segmenting medical artifact instances, they face challenges in scenarios with limited samples and class imbalances within the dataset. This challenge prompted us to explore innovative techniques such as domain adaptation using a similar dataset to enhance the classification accuracy of Mask R-CNN, a novel approach in the domain of medical image analysis. Our study has successfully established a comprehensive pipeline capable of segmenting, detecting, and classifying blood samples from slides, striking an optimal balance between computational complexity and accurate classification of medical artifacts. This advancement enables precise cell counting and classification, facilitating doctors in refining their diagnostic analyses. Improving Multi Scale Attention Networks - Bayesian Optimization for Segmenting medical images Jimut Bahan Pal and Dripta Mj The Imaging Science Journal. ( 2023 ) Abstract Article Code Bibtex Current deep learning based image segmentation methods are notable for their use of large number of parameters and extensive computational resources in training. There is a persistent need for more efficient flexible systems without compromising on precision. This work proposes a novel model that combines the best of deep learning and probabilistic machine learning to segment a wide variety of medical image datasets with state-of-the-art accuracy and limited resources. The approach benefits from the introduction of new diverse attention modules that serve multiple purposes including capturing of relevant information at different scales. These proposed attention modules are generic and can potentially be used with other architectures to boost performance. In addition, Bayesian optimization is employed to tune multi-scale weight hyperparameters of the model. The architecture combined with one of the proposed novel attention modules and tuned hyperparameters achieves the best results in segmenting ISIC 2017, LUNGS, NERVE, Skin Lesion, and CHEST datasets. Finally, the explainability of the network is analyzed by visualizing the feature map learned from the attention modules. Biomedical image analysis competitions - The state of current participation practice Matthias Eisenmann, Annika Reinke, ... Jimut Bahan Pal ... Lena Maier-Hein arXiv. ( 2022 ) Authors Abstract Article Bibtex The number of international benchmarking competitions is steadily increasing in various fields of machine learning (ML) research and practice. This holds especially true for the field of biomedical image analysis, for which dozens of competitions are organized each year. So far, however, little is known about the common practice as well as bottlenecks faced by the community in tackling the research questions posed. To shed light on the status quo of algorithm development for biomedical imaging analysis, we designed an international survey that was issued to all participants of challenges conducted in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) and the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in the year 2021 (n = 80 competitions in total). Besides questions pertaining to general information on the team and the tackled tasks, the survey covered participants’ expertise and working environments, their chosen strategies, as well as algorithm characteristics. A median of 72% challenge participants took part in the survey. According to our results, knowledge exchange was the primary incentive (70%) for participation, while the reception of prize money played only a minor role (16%). While a median of 80 working hours was spent on method development, a large portion of participants stated that they did not have enough time for method development (32%). Surprisingly, only 25% perceived the infrastructure to be a bottleneck. Overall, 94% of all solutions were deep learning-based. Of these, 84% were based on standard architectures. 43% of the respondents reported that the data samples (e.g., images) were too large to be processed at once. This was most commonly addressed by patch-based training (69%), downsampling (37%), and solving 3D analysis tasks as a series of 2D tasks. KSurprisingly, k-fold cross-validation on the training set was performed by only 37% of the participants and only 50% of the participants performed ensembling based on multiple identical models (61%) or heterogeneous models (39%). 48% of the respondents applied postprocessing steps. Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Patrick Godau, Veronika Cheplygina, Michal Kozubek, Sharib Ali, Anubha Gupta, Jan Kybic, Alison Noble, Carlos Ortiz de Solórzano, Samiksha Pachade, Caroline Petitjean, Daniel Sage, Donglai Wei, Elizabeth Wilden, Deepak Alapatt, Vincent Andrearczyk, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Vivek Singh Bawa, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Jinwook Choi, Olivier Commowick, Marie Daum, Adrien Depeursinge, Reuben Dorent, Jan Egger, Hannah Eichhorn, Sandy Engelhardt, Melanie Ganz, Gabriel Girard, Lasse Hansen, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Hyunjeong Kim, Bennett Landman, Hongwei Bran Li, Jianning Li, Jun Ma, Anne Martel, Carlos Martín-Isla, Bjoern Menze, Chinedu Innocent Nwoye, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Carole Sudre, Kimberlin van Wijnen, Armine Vardazaryan, Tom Vercauteren, Martin Wagner, Chuanbo Wang, Moi Hoon Yap, Zeyun Yu, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Rina Bao, Chanyeol Choi, Andrew Cohen, Oleh Dzyubachyk, Adrian Galdran, Tianyuan Gan, Tianqi Guo, Pradyumna Gupta, Mahmood Haithami, Edward Ho, Ikbeom Jang, Zhili Li, Zhengbo Luo, Filip Lux, Sokratis Makrogiannis, Dominik Müller, Young-tack Oh, Subeen Pang, Constantin Pape, Gorkem Polat, Charlotte Rosalie Reed, Kanghyun Ryu, Tim Scherr, Vajira Thambawita, Haoyu Wang, Xinliang Wang, Kele Xu, Hung Yeh, Doyeob Yeo, Yixuan Yuan, Yan Zeng , Xin Zhao, Julian Abbing, Jannes Adam, Nagesh Adluru, Niklas Agethen, Salman Ahmed, Yasmina Al Khalil, Mireia Alenyà, Esa Alhoniemi, Chengyang An, Talha Anwar, Tewodros Weldebirhan Arega, Netanell Avisdris, Dogu Baran Aydogan, Yingbin Bai, Maria Baldeon Calisto, Berke Doga Basaran, Marcel Beetz, Cheng Bian, Hao Bian, Kevin Blansit, Louise Bloch, Robert Bohnsack, Sara Bosticardo, Jack Breen, Mikael Brudfors, Raphael Brüngel, Mariano Cabezas, Alberto Cacciola, Zhiwei Chen, Yucong Chen, Daniel Tianming Chen, Minjeong Cho, Min-Kook Choi, Chuantao Xie Chuantao Xie, Dana Cobzas, Julien Cohen-Adad, Jorge Corral Acero, Sujit Kumar Das, Marcela de Oliveira, Hanqiu Deng, Guiming Dong, Lars Doorenbos, Cory Efird, Di Fan, Mehdi Fatan Serj, Alexandre Fenneteau, Lucas Fidon, Patryk Filipiak, René Finzel, Nuno R. Freitas, Christoph M. Friedrich, Mitchell Fulton, Finn Gaida, Francesco Galati, Christoforos Galazis, Chang Hee Gan, Zheyao Gao, Shengbo Gao, Matej Gazda, Beerend Gerats, Neil Getty, Adam Gibicar, Ryan Gifford, Sajan Gohil, Maria Grammatikopoulou, Daniel Grzech, Orhun Güley, Timo Günnemann, Chunxu Guo, Sylvain Guy, Heonjin Ha, Luyi Han, Il Song Han, Ali Hatamizadeh, Tian He, Jimin Heo, Sebastian Hitziger, SeulGi Hong, SeungBum Hong, Rian Huang, Ziyan Huang, Markus Huellebrand, Stephan Huschauer, Mustaffa Hussain, Tomoo Inubushi, Ece Isik Polat, Mojtaba Jafaritadi, SeongHun Jeong, Bailiang Jian, Yuanhong Jiang, Zhifan Jiang, Yueming Jin, Smriti Joshi, Abdolrahim Kadkhodamohammadi, Reda Abdellah Kamraoui, Inha Kang, Junghwa Kang, Davood Karimi, April Khademi, Muhammad Irfan Khan, Suleiman A. Khan, Rishab Khantwal, Kwang-Ju Kim, Timothy Kline, Satoshi Kondo, Elina Kontio, Adrian Krenzer, Artem Kroviakov, Hugo Kuijf, Satyadwyoom Kumar, Francesco La Rosa, Abhi Lad, Doohee Lee, Minho Lee, Chiara Lena, Hao Li, Ling Li, Xingyu Li, Fuyuan Liao, KuanLun Liao, Arlindo Limede Oliveira, Chaonan Lin, Shan Lin, Akis Linardos, Marius George Linguraru, Han Liu, Tao Liu, Di Liu, Yanling Liu, João Lourenço-Silva, Jingpei Lu, Jiangshan Lu, Imanol Luengo, Christina B. Lund, Huan Minh Luu, Yi Lv, Yi Lv, Uzay Macar, Leon Maechler, Sina Mansour L., Kenji Marshall, Moona Mazher, Richard McKinley, Alfonso Medela, Felix Meissen, Mingyuan Meng, Dylan Miller, Seyed Hossein Mirjahanmardi, Arnab Mishra, Samir Mitha, Hassan Mohy-ud-Din, Tony Chi Wing Mok, Gowtham Krishnan Murugesan, Enamundram Naga Karthik, Sahil Nalawade, Jakub Nalepa, Mohamed Naser, Ramin Nateghi, Hammad Naveed, Quang-Minh Nguyen, Cuong Nguyen Quoc, Brennan Nichyporuk, Bruno Oliveira, David Owen, Jimut Bahan Pal, Junwen Pan, Wentao Pan, Winnie Pang, Bogyu Park, Vivek Pawar, Kamlesh Pawar, Michael Peven, Lena Philipp, Tomasz Pieciak, Szymon Plotka, Marcel Plutat, Fattaneh Pourakpour, Domen Preložnik, Kumaradevan Punithakumar, Abdul Qayyum, Sandro Queirós, Arman Rahmim, Salar Razavi, Jintao Ren, Mina Rezaei, Jonathan Adam Rico, ZunHyan Rieu, Markus Rink, Johannes Roth, Yusely Ruiz-Gonzalez, Numan Saeed, Anindo Saha, Mostafa Salem, Ricardo Sanchez-Matilla, Kurt Schilling, Wei Shao, Zhiqiang Shen, Ruize Shi, Pengcheng Shi, Daniel Sobotka, Théodore Soulier, Bella Specktor Fadida, Danail Stoyanov, Timothy Sum Hon Mun, Xiaowu Sun, Rong Tao, Franz Thaler, Antoine Théberge, Felix Thielke, Helena Torres, Kareem A. Wahid, Jiacheng Wang, YiFei Wang, Wei Wang, Xiong Wang, Jianhui Wen, Ning Wen, Marek Wodzinski, Ye Wu, Fangfang Xia, Tianqi Xiang, Chen Xiaofei, Lizhan Xu, Tingting Xue, Yuxuan Yang, Lin Yang, Kai Yao, Huifeng Yao, Amirsaeed Yazdani, Michael Yip, Hwanseung Yoo, Fereshteh Yousefirizi, Shunkai Yu, Lei Yu, Jonathan Zamora, Ramy Ashraf Zeineldin, Dewen Zeng, Jianpeng Zhang, Bokai Zhang, Jiapeng Zhang, Fan Zhang, Huahong Zhang, Zhongchen Zhao, Zixuan Zhao, Jiachen Zhao, Can Zhao, Qingshuo Zheng, Yuheng Zhi, Ziqi Zhou, Baosheng Zou, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein Holistic network for quantifying uncertainties in medical images Jimut Bahan Pal International MICCAI Brainlesion Workshop, BrainLes 2021, Brainlesion, Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. ( 2022 ) Abstract Article Slide Video Code Bibtex Variability in delineation is an inherent property for segmenting medical imagery, when images are annotated by a variety of expert annotators. Previous methods have used adversarial training, Monte-Carlo sampling, and dropouts, which might sometimes produce a wide range of segmentation masks that differ from the styles of mask produced by a set of expert annotators. State-of-the-art method uses multiple U-Nets to capture the individual delineations, but it is computationally demanding. To mitigate this problem, a holistic network containing N-Encoder and N-Decoder is proposed, which could individually model the variability of delineation produced by the expert annotators. This will help to create segmentation masks for different tasks of the same dataset through a single network by learning the common features of multiple Encoders via a common channel and passing those features to Decoder. These create one segmentation mask. All the masks are calculated by using weighted loss at each end of the Decoders that show excellent results for some datasets. Classifying Chest X-Ray COVID-19 images via Transfer Learning Jimut Bahan Pal and Nilayan Paul 2021 Ethics and Explainability for Responsible Data Science (EE-RDS), published in IEEE Xplore. ( 2021 ) Abstract Article Slide Video Code Bibtex The internal behavior of Deep Neural Network architectures can be difficult to interpret. Certain architectures achieve impressive feats in a particular dataset while failing to show comparable performance in other datasets. Developing an architecture that performs well on a dataset can be a time-consuming affair and computationally intensive process. This study explains the effect of transfer learning by fine-tuning already available state-of-the-art architectures in different datasets and using them to classify Chest X-Ray images with high accuracy. Using transfer learning helps the model learn problem-specific features in a short period. It further shows that different models perform differently in a particular setting for a dataset. Ablation studies show that a combination of smaller structures that gives an overall better result may not give the best result in the combined model. In addition, the “belief” of the model for selecting a particular class is visualized in this study. This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.",
        "permalink": "https://jimut123.github.io/publications.html",
        "title": "PUBLICATIONS",
        "summary": "Advancing instance segmentation and WBC classification in peripheral blood smear through domain adap"
    },
    {
        "content": "Dijkstra 's algorithm is an important topic in graph theory, where we can find the shortest path between two possible points in a graph. However, we can find the paths if they are having a cost > or = 0. This is an implementation of All pairs algorithm, where the cost is > or = 0. Fig. 1: A visualisation of Dijkstra's algorithm. Source : Wikipedia . The main motive of this algorithm is to minimise the distance between two points in a map. Since this is a farely old algorithm, it is quite slow and takes a lot of space in the memory. There are several other algorithms that we can study, but this forms the basis of graph theory. Fig. 2: A visualisation of the graph of Floyd warshall algorithm algorithm. Source : Wikipedia . The basic algorithm is as follows, Note this is in layman's term: INF <- A very large number Input the number of nodes as N Initialise an array A <- [N+1][N+1] to INF A[i][j] <- 0 where i = j for K = 1 to N do: for I = 1 to N do: for J = I+1 to N do: if A[K][J] > A[K][I] + A[I][J] A[K][J] = A[K][I] + A[I][J] A[J][K] = A[K][I] + A[I][J] endif endfor endfor endfor A <- Will contain the cost matrix A[source][dest] <- Will have the cost from source to destination So let's start coding in GOD's language i.e., C++ : Let's import the necessary libraires: /* * Floyd-Warshall - All pairs algorithm, written in CPP. Author: Jimut Bahan Pal. * 4rth June, 2019. */ #include <iostream> // for input output #include <bits/stdc++.h> // for templates, just in case #include <iomanip> // for formatting and stuffs! using namespace std; // for standard namespace, else use std:: for everything! Now let's define the infinity. Here, it shall be a very big number which shouldn't be considered as an input. #define INF 999999 We now can initialise the cost matrix, by taking the numbers as input. The N will the the total number of nodes as input. We have to initialise each element in the matrix to infinity. We then have to set the elements to 0 where i=j, after that we will print it, and here's how we will do it. int main() { int N; cin>>N; int matrix[N+1][N+1]; for(int i=1;i<=N;i++) for(int j=1;j<=N;j++) { matrix[i][j]=INF; if(i==j) matrix[i][j]=0; } cout<<\"\\n Initial cost matrix :=> \\n\\n\"; for(int i=1;i<=N;i++) { for(int j=1;j<=N;j++) { cout<< setw(7)<< matrix[i][j]; } cout<<\"\\n\"; } For a sample input of a graph which have 6 nodes and 7 verteces, with cost as shown, we will have to input like this: Note: First line have the total number of verteces, second line have total number of connections and the third line have each connections, starting node, ending node and the cost. Then the initial cost matrix is displayed in the terminal like the following: 6 7 1 2 4 1 3 9 2 3 3 2 4 7 3 4 9 4 5 9 5 6 6 Initial cost matrix :=> 0 999999 999999 999999 999999 999999 999999 0 999999 999999 999999 999999 999999 999999 0 999999 999999 999999 999999 999999 999999 0 999999 999999 999999 999999 999999 999999 0 999999 999999 999999 999999 999999 999999 0 Then we have to take the cost for each of the vertex that are available as link. We will then print the matrix for visualisation. int T,i,j,cost; cin>>T; while(T--) { cin>>i>>j>>cost; matrix[i][j]=cost; matrix[j][i]=cost; } cout<<\"\\n Next cost matrix :=> \\n\\n\"; //disp_matrix(matrix,N); for(int i=1;i<=N;i++) { for(int j=1;j<=N;j++) { cout<< setw(6)<< matrix[i][j]<<\" \"; } cout<<\"\\n\"; } The output of this code segment in the terminal will be: Next cost matrix :=> 0 4 9 999999 999999 999999 4 0 3 7 999999 999999 9 3 0 9 999999 999999 999999 7 9 0 9 999999 999999 999999 999999 9 0 6 999999 999999 999999 999999 6 0 Then we will perform the main part of the algorithm, where we will optimise the cost by brute force. for(int k=1;k<=N;k++) { for(int i=1;i<=N;i++) { for(int j=i+1;j<=N;j++) { if(matrix[k][j]>matrix[k][i]+matrix[i][j]) { matrix[k][j]=matrix[k][i]+matrix[i][j]; matrix[j][k]=matrix[k][i]+matrix[i][j]; } } } } Hence, the matrix is optimised, and we print the matrix. cout<<\"\\n Final cost matrix :=> \\n\\n\"; for(int i=1;i<=N;i++) { cout<< setw(6)<< i<<\" \"; } cout<< endl; cout<< setw(6)<<\"-------\"; for(int i=1;i<=N;i++) { cout<< setw(6) <<\"-------\"; } cout<< endl; for(int i=1;i<=N;i++) { cout<< i<<\"|\"; for(int j=1;j<=N;j++) { cout<< setw(6)<< matrix[i][j]<<\" \"; } cout<<\"\\n\"; } return 0; } The final output of the cost matrix generated upon optimisation via djiktras algorithm will be something like: Final cost matrix :=> 1 2 3 4 5 6 ------------------------------------------------- 1| 0 4 7 11 20 26 2| 4 0 3 7 16 22 3| 7 3 0 9 18 24 4| 11 7 9 0 9 15 5| 20 16 18 9 0 6 6| 26 22 24 15 6 0 So we have sucessfully implemented out Dijkstra's algorithm using GOD's language. Next, we will see how to solve the same problem in a different algorithm which is quite optimised in terms of space and time complexity! Till then.... Happy coding! Comments disabled temporarily. Mail please.",
        "permalink": "https://jimut123.github.io/blogs/allPairs.html",
        "title": "All Pairs Algorithm",
        "summary": "Dijkstra 's algorithm is an important topic in graph theory, where we can find the shortest path bet"
    },
    {
        "content": "This page will have PDF's of all the writings that I will do in leisure time! Style inspired by Edsger Wybe Dijkstra's EWD scripts. 11-06-2021 - Segmentation of satellite imagery - Term Project 11-02-2021 - Generating ASCII art from images via Unsupervised Learning 11-12-2020 - A note on Inception the movie 25-04-2021 - Segmentation of Satellite Imagery 17-12-2020 - Art should entertain and educate (Thanks Seshadri) 2019 - Eisamay (Hall of fame) 2015 - Essence of Knowledge, 2015, TTIS article (Hall of fame) 2019 - Moulding my childhood 2019 - New Approach to generate, regenerate, encode and compress 3D structures using complex functions (short version of paper) 2018 - Basics of Machine Learning 17-02-2020 - How to cluster nearest unique nodes from different classes using JJCluster in Wisp application? 12-01-2020 - A Deeper Look into Hybrid Images 12-02-2018 - Deceiving Computers in Reverse Turing Test through Deep Learning 12-02-2018 - Real Time Object Detection Can be Embedded on Low Powered Devices 12-09-2018 - Botnets - A constant threat to cyberspace 31-12-2018 - New approach to Generate, Re-Generate, Encode, and Compress 3D structures using polynomial equations 28-11-2020 - Write a brief note on the Rashomon effect based on the Japanese movie Rashomon. How do you relate it to the contemporary world and your personal life? How do you equip yourself to deal with this effect? 29-10-2020 - A note on \"The Alchemist\" by Paulo Coelho 13-09-2020 - Kungfu Panda as a motivational movie 02-09-2020 - English as a global language for communication 26-06-2020 - CAPTCHA Beamer Slides 26-06-2020 - [ youtube video ] - Oral CV Presentation 25-06-2020 - Assignment - 5 (CC) 21-06-2020 - Assignment - 4 (CC) 18-06-2020 - Assignment - 3 (CC) 06-06-2020 - Assignment - 2 (CC) - ( DOI: 10.31219/osf.io/89c64 ) 22-05-2020 - Assignment - 1 (CC) - ( DOI: 10.31219/osf.io/gyk67 ) 18-05-2020 - Implementing some of the Advanced Computer Vision Algorithms - ( DOI: None ) 15-05-2020 - Camera Calibration and Fundamental Matrix Estimation with RANSAC - ( url: SSRN manuscript ) 02-05-2020 - Image stitching and Disparity 26-04-2020 - Implementing my own Cam-Scanner - ( DOI: 10.31219/osf.io/q832u ) 04-04-2020 - Detecting features via Modified Harris Corners and matching them through SIFT - ( DOI: 10.31219/osf.io/j3g9w ) 11-03-2020 - Traditional Lines and Circles detector - ( DOI: 10.31219/osf.io/sf4ye ) 20-02-2020 - ER diagram 28-06-2019 - A note on Binary Relations 16-11-2019 - Playing a class of Game using CNN with a Focus on Runner Games - ( DOI: 10.31219/osf.io/d34pu ) 08-11-2019 - Summary of Ch - 1 from India and her Problems - ( DOI: 10.31219/osf.io/j9bhv ) 04-11-2019 - How to cluster nearest unique nodes from different classes using JJCluster in Wisp application? - ( DOI: 10.31219/osf.io/32fkd ) 26-10-2019 - Reinforcement Learning - ( DOI: 10.31219/osf.io/dz6sx ) 27-07-2019 - Wisp - A preference based location finder application - ( DOI: 10.31219/osf.io/y5hk9 ) 2018 - Does exposed JSON data make paid e-songs vulnerable? - ( DOI: 10.31219/osf.io/sxenh ) 27th Aug, 2019 - Designing of search agents using Pacman - ( DOI: 10.31219/osf.io/rnsy6 ) 11th Aug, 2019 - Politics of Education in India - ( DOI: 10.31219/osf.io/qvtke ) 19th April, 2019 - MLCC SXC Week 2",
        "permalink": "https://jimut123.github.io/blogs/JBP_Scripts.html",
        "title": "JBP Scripts",
        "summary": "This page will have PDF's of all the writings that I will do in leisure time! Style inspired by Edsg"
    },
    {
        "content": "Log: Dec' 22: Updated IE-643 and CS-725 May' 23: Updated DS-899, CS-726 and CS-736 May' 24: Updated CS-772 Inspired from this and this post, I am sharing my views and materials that were generated by taking some of the courses, here at IIT Bombay, for my future reference. The materials that I am sharing here are generally my own intellectual property, like course presentations, reports etc. This is an ongoing blog and will be updated at the end of each semester. To be honest there are no such AI/ML tracks here at IIT Bombay, but one can make according to their requirements. I am enrolled here as a PhD in C-MInDS department, and for some obvious obligatory reasons, I need to do an AI/ML track which was curated by my supervisor, since he is the best in recommending courses. Deep Learning - Theory and Practice (IE-643) This is from the IEOR department, and one of my favorite courses taught by P. Balamurugan. The depth and breadth covered in this course is just amazing, exactly what is needed by a student. There are no such requirements for this course, except one needs to be comfortable with Python3 and should have some knowledge of machine learning and Pytorch. When I joined the course, I didn't have any experience of Pytorch, since I used to do everything in either Tensorflow or Keras, but I picked it along the way. This might be a bit difficult to those folks who don't like math and extensive coding, but DL/ML is all about coding, math and creating new technology. So, if you are really into Deep Learning and need to take a course, I will surely recommend this one. Another good thing about this course is, this course does not have an end-semester exam, but one needs to do a good project to finish the course requirement. To be honest, this form of teaching is necessary, since there is nothing to gain from rote learning. When creating new technology, one should know the concepts, not memorize formulas and solve sums. This course is open book too, and the questions are a bit difficult, if you don't prepare regularly. So, for me I had to do a pre-mid-sem presentation , a post mid-sem presentation an end-sem presentation along with the report of an interesting project which fetched me an AP grade. I did learn a lot from this course, and we also coded some stuff from scratch, and wrote a report on detecting camouflaged objects . The assignments were challenging and very relevant to the coursework, which challenges you to do things differently. Foundations of Machine Learning (CS-725) This course is a necessity, in case you are willing to take the Advanced Machine Learning course. This course is taught by Preethi Jyothi. The materials offered in this course are lucid and this also has an open book exam. If you are taking this course, you have to take one quiz, mid-sem exam, end-sem exam and one project, which can be done in a team of 3 to 5. There is flexibility and one can choose any topic one wishes to explore in the future. We had some tough assignments, and it took a whole week to find the best hyper-parameters. I really learned a lot, since I never expected that we need to put so much effort on just hyper-parameter tuning when building things from scratch, which the normal libraries takes care of internally. For the project, the main team members who contributed were Prateek Chanda (ex Microsoft Research) and Sandarbh Yadav (ex Oracle). I could have contributed much in this project, but I had some other stuff too, hence teamwork paid off. We made the best report and presentation for this course. This course covers the foundations of machine learning. If you are into core-statistical-ml you can take this course. Deep leaning folks need to take this course, since this covers some basic foundations which will be required by other advanced courses. Don't expect the exams to be easy, just because it is open-book, it is the reverse here in IIT Bombay, all the courses have unique question papers, which are created from previous iteration of the courses, and will compel you to think naturally even with all the materials in hand. Communication Skills (DS-899) This might look like one of the redundant courses at first glance (for it being a P/NP course), but this is a gem. At least respect the fact that the best profs in India are giving their precious time for this course for you, which they could actually utilize in their research. We had all the good faculties from each of the departments come, pitch, and assign homework. Yeah, the homework might be a bit tiring if you have other commitments like RnD and difficult courses since each of the assignments takes a minimum of 5 hours of commitment. We did learn a lot of different perspectives which trained us to become better and more skillful academicians. The course is well structured, and expect one assignment every week. Don't miss the classes since attendance is mandatory here. Here are some of the materials generated from this course: Week-1: General Proof for Hamming Codes . Week-2: Use of articles . Week-3: About Plagiarism . Week-4: Scientific writing . Week-5: Technical writing . Week-6: - off. Week-7: Reading skills . Week-8: Data representation . Week-9: - off. Week-10: Presentation . Advanced Machine Learning (CS-726) The course is taught by Prof. Sunita Sarawagi. The way she relates the explainable and classic Probabilistic Graphical Models with Deep Learning is just on a different level. This is one of a kind course specifically designed for those students who want to pursue research in Machine Learning and the advanced concepts of Deep Learning. Though this is a conceptually loaded course, one might want to learn these concepts again, even after the course ends. I liked the part where we were taught about Generative Networks, where there were several variants of it, and I would like to revisit the material once again from time to time. I think it is entirely okay not to do good in this course since stuff from the last 50 years is taught in a 4 months period, but I would definitely recommend this course once you have made up your mind to do research in core Machine Learning. Another piece of advice would be to start the project from the beginning, which we didn't (I would also like to genuinely thank my team members for saving me); we procrastinated till the last date due to various reasons and made the report and presentation the day before the submission. Here is a sample of the assignment submitted by us, and also the course presentation and report . Medical Image Computing (CS-736) This course is taught by none other than my own advisor, Prof. Suyash P. Awate. I think these links ([ link1 ] and [ link2 ]) have better reviews of the course. I highly recommend taking this course since this is one of a kind. There are a lot of evergreen classic Machine Learning topics that are essential for practicing theoretical and applied DL, which are thoroughly taught here. The only piece of advice I would like to give is to practice the materials regularly, rather than cramming up for 2 days before the exam. Things really go blank if you cram up before the night, this is a mathematically loaded course, so pay attention to detail, and don't neglect even a single line from the slide. If this is the first time you are learning about topics related to statistics and algorithms, then this course will be very difficult for you. Make sure you have the required prerequisites for the course. Also, all the good students (senior year BTechs from all the departments) take this course (they already have a fair understanding of all the topics covered in this course) hence the competition is really tough for this course (at least for me). Make very short notes and make sure you are able to re-create the fundamental knowledge of topics depicted in the slides exactly, using those notes. Deep Learning for Natural Language Processing (CS-772) This is taught by one of the most senior faculty of CSE-IITB, Pushpak Bhattacharyya, who is known for creating the foundations of NLP in India. I am glad that my guide recommended this course, and there was a lot to learn from this course. This course is taught differently, which I would ideally follow when teaching my courses, where practical knowledge is paid much attention to by giving more weightage to course projects and assignments. There is more focus on insights rather than information. He starts with foundational topics like perceptron and covers most parts of Deep Learning, which is widely studied in most courses. Since the instructor is famous for linguistics, this course tries to teach you why things are going wrong when things are going wrong inside the neural network, i.e., focus on explainability and logical analysis. The materials taught in this course are pretty common, but the instructor's insight won't be available in most of the courses. I personally think that quizzes and exams are not very important in life, focus on learning, no need to worry when you are getting negative marks, take chances and learn more. We had assignments, course projects, quizzes, mid sem, and end sem. I will recommend taking this course, and also recommend taking the beginner NLP course, which wasn't taken by me. Here are the Assignment-1 , Assignment-2 , Midsem Project and Endsem Project presentations for the course. Epilogue This ends my course list in Machine Learning. I personally feel that there is a lot to learn from. Life is just learning things continuously, and courses are just a part of that learning. New things will come and go, and those who adapt and learn to new things fast can survive. To create foundational changes in the knowledge of human existence, one must invest considerable time. I would suggest you choose a track, like going into Machine Learning, Quantum Computing, Mathematics, or Cyber Security, and invest your life in learning different topics from different sources, focusing on what you want to do rather than learning everything. Don't be like, “Oh! I have topped this course, so I must know everything in this, I don't need to learn more.” Continuously learn whenever possible and open your mind to particular ideas that interest you. Also, please take courses at your own risk... All the best!",
        "permalink": "https://jimut123.github.io/blogs/iitb_courses.html",
        "title": "My AI/ML track in IIT Bombay",
        "summary": "Log: Dec' 22: Updated IE-643 and CS-725 May' 23: Updated DS-899, CS-726 and CS-736 May' 24: Updated "
    },
    {
        "content": "During some of the most cherished days of my life, I reveled in the simplicity of being a dedicated student who relished life and spent endless hours immersed in the joy of gaming. Life was uncomplicated: I'd return home from school eager to dive into my favorite games without the burden of homework or any other concerns. The afternoons were a sanctuary of simplicity, with gaming sessions from 1:30 p.m. to 4:30 p.m., followed by either swimming classes or engaging in cricket, football, or spontaneous adventure games on the IIT KGP campus. It wasn't until the eighth grade, when we moved to the ISI Kolkata campus, that I began to contemplate the intricate process behind creating these games. My initiation into the world of gaming started in class 1 with Minesweeper and Pinball, which we played tirelessly until we discovered Road Rash—one of my earliest gaming experiences during my KG-1 days at a neighbor's house. The graphics, though basic by today's standards, held a captivating allure with their simplicity, vibrant colors, and a touch of physical realism that drew me in. From flash games offering brief bursts of gameplay to the quaint Java-written mobile games and the expansive world of PC games, my gaming journey spanned various platforms. My desire to commemorate the memories these games bestowed upon me led me to recollect over 90% of the games I've played, arranged chronologically from the early 2000s to 2016. This webpage stands as a tribute to those games that made my childhood extraordinary, as it was a time predominantly spent on either games or anime. Despite growing up, there's an ongoing quest to rediscover the same joy etched in those memories, yet the pursuit has evolved. The thrill that simple games like IGI provided, with modest graphics but a wealth of surprise and challenging levels, is hard to replicate. The transformation of gaming from single-player campaigns to multiplayer experiences has altered the landscape, and as '90s kids, we find it challenging to derive the same enjoyment from ultra-realistic environments—they lack the raw thrill of our earlier experiences, perhaps a testament to our journey into adulthood. From my extensive list, standout favorites include IGI, Midtown Madness, Vice City, and the Call of Duty series. The desire to revisit those simpler times and immerse myself in the games of my childhood is a longing to reconnect with the carefree spirit of youth.",
        "permalink": "https://jimut123.github.io/blogs/nostalgic_games.html",
        "title": "Nostalgic Games",
        "summary": "During some of the most cherished days of my life, I reveled in the simplicity of being a dedicated "
    },
    {
        "content": "I swear Windows XP was my first operating system. There are a couple of reasons to love windows like its intuitive GUI, the games you could play as a kid, the memories you made when you first used a computer. The only reason Linux isn't used as widely as windows because it doesn't comes packaged with your computer when you buy one. Trust me, no one want to change their OS, be it mobile device or Personal Computer. Linux Torvalds was once asked why did Linux Operating system failed to gain user's attention when it comes to desktop environment? The answer he gave was brilliant, He said that the people doesn't want to hack devices, it's like causing a liability issue if in case the device becomes useless. On the other side, Linux has made a tremendous effect on mobile devices because again it comes pre-packaged with the android mobile phone. The last time he read the statistics, it was more than 900,0000 users of android being added to the directory each day (Source: Google according to Torvalds). Here is a picture of OS stats captured by the cookies used in websites as shown below , which is more accurate than others. Figure showing statistics of O.S. users - Circa: 2019 (Source: Wikipedia ) The reason why I hate windows is when I started using Linux. The first reason being prone to VIRUS. I just don't understand why is it so easy to hack windows when the code is propietary. It is because almost 98% of the people use only windows when it comes to a personal computer. Most people actually doesn't buy the side softwares and uses hack version of them, which causes tremendous failures in dependency and sometimes comes free with VIRUS. Now these VIRUS are not only designed to corrupt your system, but sometimes used to gain sensitive informations like passwords when infected by Trojan or Botnets. The Ransomware is becoming a trend these days, one of my friend just caught one and he was asked to pay in bitcoin. Many people asks why Linux doesn't have VIRUS, because no one cares to make one, only 1% of the total users of the world uses Linux, and they are elite. The cost of making VIRUS is very high, and there is not one person creating it. The other thing that may have an effect may be the supportive Linux community. Most of the developers are like minded and love to share the source code of anything that they build. Windows crashes and hangs a lot. The job controller is very sophisticated and it doesn't perform the job that it is intented to perform when it should, so you have task manager to avoid that. The most frustrating thing when it comes to windows is the need to download software packages manually when developing new things. In linux it barely takes some keystroke and a good internet connection to install anything using the terminal. The Windows users are a kind of Orthodox. People may say that there is very good functionality of Adobe softwares when it comes to windows, yes thats true, but it is because you want to use windows and they will compile their builds in windows unless you shift to Linux!!! Atleast from my perspective a person who is studying Computer Science should use Linux. Most people are afraid to use linux because they think it is a Geeky stuff, well sir no, it is not, you have GUI for Linux. It takes almost 90% of the time to download stuffs in windows which can cause tremendous lack of motivation. For a noob, that's a steep learning curve, and the pain that he/she goes through will might cause him to be afraid of the frustration caused by that OS to learn new things. The most disgusting thing about windows is it's Update policy. I don't understand what is the need to download 10 GBs of update every half month. Even if you connect your net to windows, it will suck every Megabyte of it, without your knowledge, until your internet goes off. Microsoft's UI is getting retarted day by day. What is the problem with the old UI? I asked most of my friends and their personal favourite is Windows 7 and Windows XP's UI. There are a several other reasons to hate Microsoft when it comes to a more higher perspective and when it comes from the company policy perspective (credits: Dr. Richard Stallman [SO LET ME JUST PLAGIARISE THE POINTS]): Microsoft is running a patent protection racket , threatening to sue users of free software. Microsoft's principal wrong is distributing a nonfree operating system , Microsoft Windows. That system is jam-packed with malicious functionalities , including surveillance of users, DRM, censorship and a universal back door. Microsoft tricked users into \"upgrading\" to Windows 10. Microsoft Windows 10 forced software changes can sabotage the user terribly if Microsoft chooses an inconvenient time to do them. Since the article is in the mainstream media, it suggests only to buy another computer that serves a master that doesn't do this particular form of nastiness. It completely ignores the possibility of installing a free operating system in the PC—which doesn't even require buying a new computer. Microsoft tablets and phones impose censorship of applications . Microsoft's chatbot in China threatens people who communicate using prohibited words . Microsoft recorded users of Xboxes and had human workers listen to the recordings. Morally I see no difference between having human workers listen and having speech-recognition systems listen. Both intrude on privacy. Microsoft forced a ridiculous \"open\" standard, OOXML (used in DOCX files), through the International Standards Organization by corrupting most of the national standards organizations that voted. The specifications document was so long that it would be difficult for anyone else to implement it properly. When the proposed standard was submitted through the usual track, experienced evaluators rejected it for many good reasons. Microsoft responded using a special override procedure in which its money buy the support of many of the voting countries, thus bypassing proper evaluation and demonstrating that ISO can be bought. Microsoft pressured nearly all manufacturers of PCs to pay for a Windows license for every machine sold, thus charging every purchaser for a Windows license . This is referred to sardonically as the \"Microsoft tax\". (Wherever that page says \"Linux\" it actually means the GNU/Linux operating system rather than Linux proper.) The fee doesn't force you to run Windows on your PC, but it is an injustice nonetheless. One way to avoid it is to buy hardware that is never sold with Windows . Some countries have laws under which users have sued for the right to get a reimbursement for the Windows license. Exercising that right is a hassle, but doing so is useful as it puts pressure on the system Microsoft has set up. However, the existence of an inconvenient escape path, limited to a few countries, has no effect on the judgment that Microsoft's practices are an injustice and deserve condemnation. If you are reading this and you are anti-open-sourced, then here are some more nuisance for you to make your blood warm (credits: Aniket ): What is the similarity between a computer and an AC? - Both becomes useless when windows are opened. There's a reason there is \"Linux on Windows\", because Windows is not meant for \"sane programming\". By the time a windows user starts writing code after updating the OS and installing 200gb of software, a Linux user will have moved on to his 5th failed startup.",
        "permalink": "https://jimut123.github.io/blogs/hatingmicrosoft.html",
        "title": "Why I prefer Linux over Windows O.S.",
        "summary": "I swear Windows XP was my first operating system. There are a couple of reasons to love windows like"
    },
    {
        "content": "Academia or Corporate What's My Future Career Choice? This is a question I often get from my peers. Over the years, my perspective has evolved, and I now lean more toward academia. Why? I'm a B.Sc. graduate, which technically means I'm not an engineer. So, pursuing academia seems like a more likely path. What's driven me so far are “willpower” and “hope”—the belief that I can solve any problem if I dedicate enough time and effort. The reward that comes from overcoming these challenges makes it all worthwhile. Someone once asked me: If you could go back two years, with all the experience you've gained from your PhD, would you still choose to pursue it? Without hesitation, my answer is Yes! Since childhood, I've been fascinated by “mad” scientists. Characters like Dr. Heinz Doofenshmirtz, Phineas and Ferb, and the Japanese series Kiteretsu were early inspirations. These characters weren't just engineers; they were S-rank scientists who used their basements to create disruptive technologies capable of changing the world. There was always something superior about the so-called \"villains\"—their inventions fueled the heroes' journeys to stop them. After all, it's the “madmen” who create the extraordinary inventions that drive progress. On the other hand, highly talented scientists often operate with a strong sense of philosophy. They create technology driven by intellectual curiosity combined with art and imagination, which can lead to peace or war. They create because they must, and no one else can. Scientists like Andrej Karpathy have been motivated by sci-fi games like Half-Life from a young age, which have also highlighted the greatness of science. While I don't endorse the unethical actions of figures like Shirō Ishii, I believe that doing science ethically is both meaningful and fulfilling. War has indeed historically accelerated technological innovation—think of World War I, where tanks dominated, and World War II when planes transformed the landscape of destruction. But should we rely on conflict for progress? Is that how the human race has evolved to be? Why Not Corporate? I don't feel inclined toward a corporate career at this stage of my life for several reasons. First and foremost, when you develop something in the corporate world (e.g., as a software developer), you rarely get personal recognition. The company owns your work, and you contribute just a fraction of the code. While your contributions may be widely used, you lack ownership and visibility. Your name won't be remembered unless you work in open-source, like Linus Torvalds or David Heinemeier Hansson. Some corporate jobs offer higher pay, work-life balance, and enjoyable weekends. But eventually, the 9-to-5 grind becomes monotonous. As the saying goes, even if you eat a delicacy every day, it becomes bland. The excitement fades over time, especially in your 20s and 30s, leading to intellectual and personal stagnation. Your growth becomes dependent on the company's direction. Moreover, if you work on proprietary tools, you can't quickly transfer those skills to other jobs, especially after signing NDAs. In contrast, your name is attached to the papers you publish in academia. You control your research, and your knowledge grows exponentially. A well-published paper can earn you recognition for years. In academia, you must constantly update your skills, but that's a part of intellectual growth, unlike in corporate, where you're boxed into specific roles. Doing an average corporate job is much worse than being an average professor in academia. While corporate jobs pay more, an excellent academic can eventually out-earn software developers at top companies. Research positions at giants like Meta, Google, or Microsoft differ but often have limitations. Your research must align with their interests, and research teams are frequently the first to be cut during resource constraints. Academia, on the other hand, fosters continuous intellectual growth. There are so many sub-domains I want to explore, and academia offers the flexibility to do so. Additionally, you meet new people through conferences and collaborations rather than seeing the same faces daily in a small team. The routine of a 9-to-5 job would be draining for me. I need the flexibility to work on my own terms, without rigid oversight, as long as I produce results. The Challenges of Research Most of today's research follows trends rather than establishing new foundations, driven by the pressure to publish quickly. Foundational research takes years and often fails, like Geoffrey Hinton's work on Restricted Boltzmann Machines before developing backpropagation. In research, you sometimes must choose between what works and what's experimental but might fail. Unfortunately, companies push research that profits them rather than what advances knowledge or human curiosity. This is the reason, a novel idea may take time to be accepted by the community, and researchers push themselves in the general hype, which creates a bubble/trend that changes over time. Some people will dislike your work in academia simply because they don't see their reflections on it. Others will appreciate the “art” of your research. You will face rejection, even after giving your best effort. But you have to keep pushing. The eyes of an artist are needed to appreciate art, and most people are not artists—they follow trends or what's popular. People outside your field may criticize you, but that's part of the process. Reviewers, too, may be less qualified than you, but they can reject your work. There are flaws in the review system, such as double-blind papers being recognizable when authors post on arXiv. Certain groups or styles are more recognizable and exclusive, creating bias. However, the pursuit of knowledge must continue despite these flaws. The Cost of Academia Choosing academia requires sacrifices. As my faculty advisor, Prof. Pushpak, once said, “In academia, you'll see your corporate friends getting promoted, married, earning triple your salary, enjoying weekends, and traveling—while you're working hard, often on weekends, before deadlines, refining work that only a few will recognize.” It can be demoralizing, but your hard work will pay off in the long run. After your PhD, you'll be in a better position, and those years of sacrifice will make sense. Academia is full of highs and lows, but persistence is vital. From a young age, the education system has often felt broken. I wonder if I would have been better off focusing on the philosophy of the subjects that I was learning rather than memorizing its contents for the sake of marks and degree certificates. Interestingly, the education system has put the younger generation into this rat race of marks and degrees, leading to the devaluation of education. The core philosophy of the education system should shift from the pursuit of \"high rankings\" to a curiosity-driven, student-centered approach that focuses on nurturing each student's growth, free from unnecessary peer pressure, with the goal of advancing humanity. But your love for research should have boundaries or you will end up like those who have sacrificed everything for the sake of research. Think of those scientists who gave up everything for science, like Nikola Tesla, who once said, “All these years I have spent in the service of mankind brought me nothing but insults and humiliation,” or Grigory Perelman, whose dedication and sacrifice changed the world. The world is forever evolving, and the progress of the human race is based on change. Change in perspective, change in ideology, change in technology, etc. Like previously, we were hunter-gatherers, but now we have created a civilization that works in a coordinated way, which is the main reason the human race has survived till now, even when facing large-scale wars, disasters, famines, etc. As the Sanskrit shloka says, \"स्वदेशे पूज्यते राजा, विद्वान सर्वत्र पूज्यते,\" which means a king is worshipped in his own country whereas the “Pundit” /scholar is worshipped everywhere he goes. Do you want to be a corporate king who gets worshipped by his juniors for promotion, or do you want to be a scholar who is recognized worldwide? Regarding 70/90 hours work week The idea of working 70-90 hours a week is baffling. Why would anyone commit to such grueling schedules for a company where their gains are linear, while the company enjoys exponential profits? How does this help someone grow intellectually? Why work for an organization that treats its employees as expendable, keeping the profits for itself? Isn't this a form of betrayal? The focus shouldn't just be on the quantity of work but on the quality. We're humans, not machines. Creativity thrives in balance, and no matter how interesting a task is, doing it endlessly can make it monotonous. What's the point of fixing the mistakes of previous generations while they sit back and demand we save them? It feels unfair—like being commanded by those who failed to act when they had the chance. Take China, for instance; they've surged ahead because they acted when the time was right. And now, the same people who missed that opportunity want us to overwork for them, all while they enjoy disproportionate pay and comfort. This imbalance is ridiculous. It's no surprise that many workers leave India for better opportunities. The culture of valuing quantity over quality, and the lack of prestige for great performance, drives talent away. Here, it feels like you're paid not to grow, but to let your creativity and intellect stagnate—to become a cog in a robotic system. Special thanks to Chandan, Sourav Rout and Nilayan for their valuable feedback. Also, images are collected From various sources from Google and some are generated, sorry for not finding the references!",
        "permalink": "https://jimut123.github.io/blogs/academia_or_corp.html",
        "title": "What to choose - Academia or Corporate?",
        "summary": "Academia or Corporate What's My Future Career Choice? This is a question I often get from my peers. "
    },
    {
        "content": "From Naruto Madara Uchiha Madara Uchiha is a GOD. He is the only being who has no weaknesses, hence even the writer didn't know how to kill him. Here is the speech which I truly admire: Wake up to reality. Nothing ever goes as planned in this accursed world. The longer you live, the more you will realize, that the only things that truly exist in this reality are merely pain, suffering and futility. Listen... Everywhere you look in this world, wherever there is light, there will always be shadows to be found as well. As long as there is a concept of victors, the vanquished will also exist. The selfish intent of wanting to preserve peace, initiates wars. And Hatred is born in order to protect Love. There are nexuses, causal relationships that cannot be separated. I want to sever the fate of this world. A world of only Victors. A world of only Peace. A world of only Love. I will create such a World. I am... the Ghost of the Uchiha. For truly this reality... is a Hell. The fact that he fought the whole shinobi world along with all the tailed beasts single handedly shows the power and determination of this character. He is the founder of the Uchiha Clan, which runs the show. Madara Uchiha Madara Uchiha Madara Uchiha My take from this anime is there are no villains and heroes in this world. We admire heroes as kids, but as we grow old, we learn to understand the philosophy of villains. After all, everyone is born with a pure heart. Everyone is controlled and exploited by some other's decision in one or another way throughout life. Even if you get everything in the world, which you desire, you won't be happy. There is nothing as perfectness or utopian world. He who walks alone walks the fastest. How much you grow, there will be someone who surpasses you in everything. There's a saying: Roses are red. Violets are blue. No matter how hard you try, an Asian will be better than you. Someone's life's work might be someone's weekend's work, it all depends. Success is relative. It might thrill someone for achieving something, but other may consider such achievements meaningless, he/she may have such achievements for breakfast. The greatest men are unnoticed. Being the best has their own set of consequences, which might not always be good. Ultimately, everyone is cursed by their own destiny and fate, which are unavoidable. Whatever happens will happen eventually, how much hard you try to stop it. Karma is a real thing, and destiny cannot be avoided. One who learns to master to control his/her mind wins the world. Happiness, health and peace of mind are the things that a person needs to chase, not money or fame, because everything is temporary. Humans have created this idea of a society in such a way that everyone is bound to live within its norms. Humans have made themselves the only thing that has meaningful existence in this universe, which might not be true. We are specks of solar dust anyway, and our existence is merely a process of manipulating atoms. The power balance in everyone's life remains almost the same. It's extremely unlikely that a person is extremely well in both sports and academics. He/She may be very good in academics but may suffer from disease. It is necessary to find happiness in nothingness. One who has a bad and childhood full of hardships, usually has a good adulthood with minimum effort, since he/she has learnt the way world works, and an unknown power works in support, protecting the balance. The reverse is also true, hence everything in this world is balanced. Getting everything and knowing everything beforehand can make the present very boring. Also, as a character, Naruto was very dumb in the beginning. It's not always to set out bias towards the outstanding ones. The real fun is when a guy with average intelligence surpasses everyone just by rigorous training and by the will and determination of never giving up. Itachi Uchiha He is the prodigy who could have surpassed Madara Uchiha. His sacrifice for the village will be remembered. He thought like a Hokage at the age of 7. Here are some of his quotes: Even the strongest of opponents always has a weakness. Knowledge and awareness are vague, and perhaps better called illusions. People live their lives bound by what they accept as correct and true. That's how they define Reality. But what does it mean to be “correct” or “true”? Merely vague concepts… Their Reality may all be a mirage. Can we consider them to simply be living in their own world, shaped by their beliefs? Everyone lives within their own subjective interpretation. Teachings that do not speak of pain have no meaning, for humankind cannot gain anything in return. I actually don't think something like perfection exists. That is, I think why we are born able to absorb things... and by comparing ourselves with something else we can finally head in a good direction. It is not wise to judge others based on your own preconceptions and by their appearances. Those who cannot acknowledge themselves will eventually fail. Itachi Uchiha Uchiha Clan To be honest, all the strongest and the prominent members of Uchiha are cool. They possess extraordinary power which only the Senjus can rival. Clan of the Uchihas Akatsuki These are probably the coolest villains in the anime history. They are so cool that even two members can destroy a single village. They were mainly assigned to capture the tailed beasts, which probably a whole arsenal of any village will fail to achieve. All the members of Akatsuki are at Hokage level and are extremely professional and efficient in completing their tasks in time. Akatsuki Madara never said that From Akame ga Kill! Esdeath Well, I thought I will put Gojo before Esdeath, but it wouldn't be fair. She is the empire's strongest and coldest character. Probably the best female character in the anime world. She brings back memories of someone whom I have seen in real life :) The saddest part of this anime is all the best characters dies at the end, including Esdeath. She leads the Jaegars team. Esdeath Jaegars From Jujutsu Kaisen Gojo Satoru Overpowered Kakashi, level 1000 boss who is replaying the game just for fun. He is the strongest sorcerer and the strongest character in the jujutsu world. My man just woke up one day, and started to speak facts. Some of his famous quotes are: Dying to win and risking death to win are completely different. When Granted Everything, You Can't Do Anything. But no matter how many allies you have around you, when you die, you'll be alone. Love Is The Most Twisted Curse Of All. Gojo Satoru Yuta Okkotsu Gojo's cousin, seriously depressed, but super strong character. Yuta Okkotsu From Chainsaw Man Makima aka 'Control Devil' Being the charming character, one might think she is saving the humanity, since she is with the good-side, but the truth is she is the real antagonist of the series, and one of my favorite characters. Her powers are unknown and of unimaginable magnitude. Makima From Demon Slayer: Kimetsu no Yaiba Giyu Tomioka Giyu Tomioka From Death Note Ryuga Hideki Super smart, but dies just because he was friend with his murderer. He is a prodigy and I would recommend Death Note to everyone. Here is his famous saying There are many types of monsters in this world, monsters who will not show themselves and who cause trouble. Monsters who abduct children, monsters who devour dreams, monsters who suck blood, and, monsters who always tell lies. Lying monsters are a real nuisance, they are much more cunning than other monsters. They pose as humans, even though they have no understanding of the human heart. They eat, even though they've never experienced hunger. They study even though they have no interest in academics. They seek friendship even though they do not know how to love. If I were to encounter such a monster, I would likely be eaten by it, because in truth, I am that monster. Ryuga Hideki aka 'L' From Classroom of the Elite Kiyotaka Ayanokoji “Genius lives only one storey above madness.” This guy was created in the white room (a mysterious organization), where you sacrifice childhood to be an exceptionally talented student of the country. He has displayed tremendous ability in every field he was trained in since childhood, right from studies to sports. To experience how the world works in real life, he has been admitted to a school where he chooses to remain average. “It takes great talent and skill to conceal one’s talent and skill.” He doesn’t have such social skills as other candidates of the class, but the things that he does without them are truly exceptional. He is a true genius and stays to remain average, to lie below the radar, where in real life, he is the best student the system has seen. He rarely takes part in the front lines and controls the class from behind. Kiyotaka Ayanokoji From Noragami Yato Yato with his regelia Yukine From One-Punch Man Saitama This character shows us that for being super strong, one need to work tremendously hard. He worked so hard that all this hair fell off. He probably is the strongest hero, but he stays at the bottom for specific reasons. In exchange for power, maybe I've lost something that is essential to being human. Saitama From Maou Gakuin no Futekigousha (The Misfit of Demon King Academy) Anos Voldigoad Anos tells us that everything depends on the user, not weapon. For example, if one is an expert, he doesn't have to use legendary weapons for doing extra-ordinary damage, they can cause extra-ordinary damage with normal weapons. Similarly, a naïve user cannot cause extra-ordinary damage even if he is given legendary weapons. He being the lord of the devils, and enormous powers, shows substantial charm from episode 1. Anos Voldigoad From Mob Psycho 100 Shigeo Kageyama Resembles saitama but has mistic powers. Kageyama From Tokyo Ghoul Rize Kamishiro The anime takes its turns. I would not recommend this to anyone except the first and second seasons. She is the source of power for Kaneki Ken, since her Ghoul parts were transplanted to Kaneki Ken, the protagonist of the story. Though Kaneki took her powers to new heights, by training and patiently enduring. There's no such thing as fate. It's simply a combination of one circumstance and the next. There are times when you have to give up on one thing to preserve the other. Rize Kaneki Ken He wanted to make sure that ghouls and humans co-existed peacefully. He is one of the strongest members of the Aogiri tree. Kaneki Ken From Plunderer Licht Bach The strongest member of the 7 aces. His true powers are still unknown. Licht Bach From Beyblade Kai The cool chartacter that we admired during our childhood. He was the strongest side character and probably the most loved character because of his attitude. Kai From Charlotte Yu Otosaka Yu From Case Closed Detective Conan The smart kid that we all wanted to be sometime in our life. I am not sure how long the episode chain will continue, but looks like he will not age yet. He was drugged to a younger form but he had not been able to find his culprits yet. Detective Conan Note: The pictures are not mine and were collected from different sources available on the internet.",
        "permalink": "https://jimut123.github.io/blogs/anime_op.html",
        "title": "My favourite Anime Characters from every anime I have watched",
        "summary": "From Naruto Madara Uchiha Madara Uchiha is a GOD. He is the only being who has no weaknesses, hence "
    },
    {
        "content": "Ever heard Elon Musk saying We're Probably Living in a Simulation ? Ever thought yourself whether you really live inside the simulation or not? I felt that we all live inside a simulation, and to be honest there are several limitations of the simulation. Ever wondered what a little snail would think in the middle of the ocean given that it will only travel a maximum of 10 km radius in its lifetime? It will think its infinite. But is ocean infinite? Are we not smaller than the snail if we compare the scales of ocean: snail and humans: solar system (oops' we haven't gone outside the solar system yet). Is it not a limitation of human being to not make a technology that cannot travel in the speed of light? Yes... even if we travel at the speed of light, it will probably take 105,700 light years to explore just a linear manifold of a galaxy. Probably humanity will be over by then if even we make something feasible as a locomotive with all necessary condition for survival. Let's think of the solar system, isn't it a natural locomotive moving at the speed of 490,000 miles per hour ? Okay lets think of a limitation to the capability of human brain system. Let's consider the solar system only for our simplicity. Now, we remove Earth from the solar system, we are left with the sun and probably the uncountable little debris and some planets, and sun, right? Now we remove the planets, we are left with only the debris and sun, now we remove the debris we are left with the sun and now we remove the sun we are left with a very big darker than you expect space (remeber we are only confined to the solar system for now, not considering that would give us light from the stars). Now if we remove the vaccuum space too, what are we left with? can you visualize? is that filled space or something else? Even if we remove that something else what are we left with? Isn't this like the simulation where our brain is only capable of understanding and visualizing the things that are only present in the solar system? (or probably a subset of the space inside the solar system). Did you ever have Deja Vu? Our brain has an enormous processing power, we all know that, right? Sometimes human beings observe too much unconsciously and the numbers and information is continuously getting crunched in the brain. Now, sometimes we may even have dreams about something that might not have had happened or seen by us but when we encounter it in the so called real life, we think oh we have seen this before.... looks familiar. This happens to rarest of the people that I have encountered with and even me, the exact same settings that I have seen in my dreams, comes in the reality when I am able to connect them. Looks like we are looping in time but we are not. What if we are all part of a simulation controlled by higher dimensional beings, we will cease to exist once we die. Ever thought of the fact that life is mostly abstracted? What will happen after you die? Is there is something as infinity? Are there aliens? Who let the dogs out? Was the moon landing faked? Is there something about aliens in the area 51 etc. all these thoughts are what we might call as abstraction. What if life on Earth is actually easy and we cannot live in the world where the higher dimensional being controls us. Maybe life is hard there, and maybe time is different there, if even time exists... What if everything you did, experienced, and gone through was just a simulation? Hard to believe right? Yes... but it might be... there is a high chance of being so. Everyone as a child had these questions, how did we come to Earth? Oh, it's the organic decomposition of chemicals to create a holistic and living tissue and then a living organism. What exactly is present in the structure of the chemical composition that makes it conscious and living? Why is it hard for non-living things to move even if their structure and chemical composition is much more similar to a living organism? Is there something as soul or something as death? What if the people who are considered as the messenger of gods or gods who had possessed super human being qualities had broken some part of the simulation which makes them defy the simple laws of the solar system? Also, why would this not be a simulation? Humans have restricted powers, even in terms of thinking and hacking the simple laws of nature which takes them several years to come up with the laws of nature. If we can develop such complex vision system, such complex hearing and thinking system all by ourselves, why can we not transfer the contents of one mind to another? That might be a reason that human race will evolve at a faster pace than it is supposed to and may eventually break the simulation of perhaps be aware that we are living in a simulation? Perhaps life on Earth is easy, but what will happen after we die? What is actually consciousness and why can't machine replicate it? It will be good for robots to exchange consciousness and perhaps have better efficiency by replacing their own part all by themselves? Will that be capable of next generation organisms? Atleast will be better than humans, they have consciousness, they can even progress to Kardashev scale - 5 level civilization and might eventually be god. What if certain places you visit might be already similar looking to a certain place which you think you have visited and is somewhat similar to the thing that you might consider is similar? Is that similar to a compression algorithm that the simulation is using? Well, if things are discrete then there will be certain amount of limitation to the system in which you might find that the thing needs to be compressed, but real world has things in Real which is uncountably infinite... but is it? To what extent does the human perception cannot distinguish between two different distinct objects? Are we not a part of a larger computing entity? since every event is dependent in this world and things like butterfly effect also exists? What if much of the outer space that we explore using telescope is just a false canvas? We might not ever have the capacity to go and check whether that exists or not? or perhaps the things unlock only when we go to a certain vicinity of the object? Humor: What is the probability after reading this article that you might be eliminated from this simulation with all your memories deleted from every entity of this simulation, which will cease to prove that you ever existed? Well you won't be eliminated if you think that you might not live a simulation which I might also think to some extent for which reason you are still reading this... P.S.: It's better to take these thoughts lightly, nothing will change. Also, live your life without complaining... What if you go to a void of subconscious dark space after you die where you can't do anything?",
        "permalink": "https://jimut123.github.io/blogs/crazyTheorySimulation.html",
        "title": "Crazy Creepy Theories",
        "summary": "Ever heard Elon Musk saying We're Probably Living in a Simulation ? Ever thought yourself whether yo"
    },
    {
        "content": "Here I will share my notebooks which was made during Machine Learning Course. These are mainly focussed on artificial data generation and fundamentals of Machine Learning. This is an ongoing blog site and I will be updating this a lot. 0. Colab Tricks Dated: 16.10.2020 ,Various Examples of tricks using Colab. See Notebook 1. Linear Regression Dated: 16.1.2020 ,Various Examples of Linear Regression using custom generated dataset tried. See Notebook 2. Logistic Regression Dated: 26.1.2020, Various Examples of Logistic Regression using custom generated dataset tried. See Notebook 3. Polynomial Regression and LASSO Dated: 6.2.2020, Regularization using LASSO used to reduce overfitting. See Notebook 4. K-Nearest Neighbour Dated: 16.2.2020, A taste of Clustering Algorithm. See Notebook 5. Neural Networks using Keras Dated: 16.3.2020, Various experiments using Keras See Notebook 6. CNN Transfer Learning Dated: 17.4.2020, First taste of Transfer Learning using Keras. See Notebook 7. Autoencoders and its' variants Dated: 11.10.2020, Denoising, Reconstructing, and other autoencoders. See Notebook See Notebook 8. DCGAN Dated: 06.11.2020 DCGAN on FASHION MNIST dataset. See Notebook",
        "permalink": "https://jimut123.github.io/blogs/machine_learning_course.html",
        "title": "Practical Machine Learning",
        "summary": "Here I will share my notebooks which was made during Machine Learning Course. These are mainly focus"
    },
    {
        "content": "IBM Watson is the \"state of the art\" solution provider for AI. Watson caught the media and industry attention when it played and won $77,147 (£47,812) in Jeopardy back in 2005, without connecting to any internet and just training on a plethora of text knowledge base. It's Christmas time and I wanted to gift myself an AI, just kidding, I am not iron man or stuff, learning NLP and training a model from scratch is a pain in the neck. I wanted something very cool which can take very less effort and can make something which suits my need. I started with IBM watson assistant and was surprised by the fact that almost any kind of assistant can be build without using any programming. Well, I am a programmer, and it did hurt my feelings (LOL). I am a little dissapointed that IBM is providing so much freedom to create almost anything for almost anyone without knowing the underlying bits and bytes of the architecture and principles in which the thing is to be designed. IBM is a pioneer in the field of machine learning and AI and has the top-notch R&D team in the world. They are the one who worked with speech recognition from an early age before anyone knew it existed. But it also have a downside, the learning curve for learning IBM studio can be very steep and sometimes careless mistakes may cause unneccessary results, and banging your head several times wouldnot solve your problem, but to try over and over again will. There are a very few bugs in the Watson cloud too, but they are minor, and mostly related to API errors. So lets see what I have build using IBM watson! My first creation was a very simple flower shop assistant. I learnt a lot about \"entities\", \"intents\", and \"dialogues\". Intents are keyowords that are associated to the watson assistant when we type or speak something in there. The intent specify the underlying meaning and the purpose of the statement which is produced to the assitant. They begins with # sign. Like for example #goodbye is an intent, and when we find any kind of sentence which is similar to that, then we may send appropriate replies. Entities are like particular queries that are related to some enities which has to produce very specific replies. Dialogues are mostly the flowchart of the decision which needs to be taken when you encounter something, as shown in Fig. 1 . I also learnt about context and digression. Context helps to memorize the needed context in a variable, upon which the follow up questions would be answered like humans by the robot. Digression is something similar to deviation, like for example say that if a person asks about something and the chatbot needs more information to answer the question, in that case the chatbot may question for follow up question to the users and so upon answering that question the chatbot will answer the user's question, it may so happen that the user doesn't answer the question and asks a different question, in that case of a digression, the chatbot will memorize that a unanswered questioned needs to be answered and so it will perform simultaneous ansewering of questions alongside asking for the necessary information everytime from the user. Fig. 1: A layout of the dialogue flowchart using watson assistant After working and designing the IBM watson assistant it's time to deploy! Here you can see the actual deployment of the watson in a wordpress site. A very minimalistic version of a flower shop assitant chatbot. It is said that Chatbots are becoming a need in the industry where the customer care staffs are just getting frustrated by the repition of the same question that they are being asked several times a day. The main fun thing about this chatbot is it will answer all your basic and silly queries about the flower shop as much as possible but when it comes to more difficult questions it will direct you straight to a \"virtual employee\" in case if your answer needs creativity to answer. The watson assistant has almost 10,000 api calls limit for one month when it comes to a default free plan but above that it will charge you some money. It is said that the industries are blooming and it is a cost effective method to apply something like a chatbot to your solutions since this will lead to saving of time, effort and money which the eployees can use in some creative tasks. The demo of trial in studio and wordpress deployment of the assistant is shown in Fig. 2 , Fig. 3 and Fig. 4 . Fig. 2: A demo of the watson assistant in studio. Fig. 3: Another demo of the watson assistant in studio. Fig. 4: A demo of the watson assistant in wordpress site. After all these hard work, it was time to celebrate! (Watson: We don't do that here). I created another application using Watson visual recognition and using all the tools provided by the deepbluemix. Here is the demo of that application , also shown in Fig. 5 . The purpose of the vision application is very simple, it detects furniture, i.e., table, chairs and beds. It doesn't return anything if you give some negative examples like a basketball court, or maybe a picture of animal. Fig. 5: A demo of Watson vision application. At last here is a bonus picture for you, Be happy, keep smiling, and happy learning! Fig. 6: Some of the tricky images which even the humans find confusing to classify! LOL.",
        "permalink": "https://jimut123.github.io/blogs/workingWithWatson.html",
        "title": "Working with IBM Watson",
        "summary": "IBM Watson is the \"state of the art\" solution provider for AI. Watson caught the media and industry "
    },
    {
        "content": "June 30, 2017 - Owl City - Adam Young . June 24, 2016 - CREATING AN IMAGE FROM SCRATCH . June 24, 2016 - STEGANOGRAPHY . June 24, 2016 - MODIFYING AN IMAGE .",
        "permalink": "https://jimut123.github.io/blogs/codepen.html",
        "title": "Old Stuffs from Codepen",
        "summary": "June 30, 2017 - Owl City - Adam Young . June 24, 2016 - CREATING AN IMAGE FROM SCRATCH . June 24, 20"
    },
    {
        "content": "In the budding stages of their relationship, the girl discovered the boy's diary, where he had poured out his innermost thoughts: \"In my youth, I had an affinity for all things new—books, toys, friends. However, as time advanced, my preferences evolved. I came to appreciate the nostalgic scent of old books over fresh ones. I transformed discarded toys from my basement into new creations and endeavored to mend the lives of those misguided by unfortunate events. Making new friends became a rarity, reserved for those emitting a similar aura. The act of reviving and crafting from unused or broken parts brought me immense joy. It felt like breathing life into neglected entities that craved purpose. Fixing people, though messy at times, seemed worthwhile. Repairing replaceable items, like a broken umbrella, by hand reflected my genuine care for my belongings. To me, everything possessed a life of its own. Perhaps that's why my possessions stop working once I pass them on to someone else. I sense life in things, devoid of consciousness, and believe they too experience love. However, in matters of human connection, especially romantic involvement, broken individuals gravitate toward me. I've never been someone's first love, and once fixed, people tend to leave. Mending a broken heart parallels repairing a cracked glass vase—it may seem restored, but the fracture remains. Fixing is transient in this existence, and a pure soul, once cracked, cannot be held together indefinitely. Is this why I've remained single for five years? People depart, yet inanimate objects endure. Unlike living beings, things don't betray you. Their value often goes unnoticed until desperation strikes. I question whether my attraction to simplicity will hinder my ability to relish life's extravagances. The few things I genuinely love—my research work, functional code, and family—have become my pillars. Activities I once enjoyed, like playing computer games, have lost their allure. If this pattern persists, I may end up alone after losing my close ones. Finding individuals who share my sentiments and emotional values is rare, and I cherish those connections. While being poor is one thing, wealth without meaningful connections feels isolating, as if no one truly understands or cares.\" Suddenly, the boy entered and found the girl in tears. He expressed, moved by the narrative, \"Huh? I copied this from some forgotten website.\" Concerned, the girl responded, \"I was worried about you; I thought it was about you.\" The boy chuckled, \"Nopes, why would it be me? Just a complete loser soul… lol.\" The girl smiled, bid farewell, and on the doorstep lay the meticulously stitched umbrella.",
        "permalink": "https://jimut123.github.io/blogs/broken_soul.html",
        "title": "Echoes of Broken Souls",
        "summary": "In the budding stages of their relationship, the girl discovered the boy's diary, where he had poure"
    },
    {
        "content": "During a party at C-MInDS (November, 2022), I was asked to tell a joke by Prof. Sunita Sarawagi. Well, I handled the situation quite well, at least they laughed, not sure, probably the joke was too lame. Hence, I am keeping this page for accumulating all the ridiculously funny experiences that I will have/ recall, so I can make them genuinely laugh, during such future parties. This page is for easy reference to all the stupid funny things that happened in my life. This page will link to other pages, in a retro fashion, was probably motivated by this page here . The lazy Professor The confused student",
        "permalink": "https://jimut123.github.io/blogs/lol.html",
        "title": "LOL",
        "summary": "During a party at C-MInDS (November, 2022), I was asked to tell a joke by Prof. Sunita Sarawagi. Wel"
    },
    {
        "content": "Introduction Convolutional Neural Network (CNN) can do a lot of things that were previously considered almost impossible (I am taking about 1960's here). After the invention of CNN for digit recognition by Yann Lee Cunn et al. , deep learning and Machine learning have gained a tremendous boost in almost every nook and corner of IT industry. Machine Learning is itself a very old topic, but earlier, there were no computational power to manifest such enormous computation and store such huge quantities of data. Due to the growth of hardware technologies (and constant growth by Moore's Law), there is a huge demand for this subject in recent industry. A person who is qualified enough to solve Machine Learning tasks are paid almost double, to those who are just software engineers. Almost every IT department seeks an analyst who can crunch information from the data to predict the future and make the company aware of the policies that it needs to make. One of the greatest development in the field of Machine Learning is image recognition. Though there are several algorithms which focusses on playing of games and particular use of deep reinforcement learning, our experiment explores the concept of a rather simple model of automating games. The model that we have used is developed over the years by eminent researchers by trial and errors. We have tweaked some of the parameters and shown that just by seeing and learning, the computer can perform a lot of tasks which can automate many things in day to day life. Video 1: Beamer presentation for Communicative English Course on this topic. A CNN when applied in image recognition, acts on an image. That is, if we want to apply something using CNN into video, we have to extract the frames, and work on it. The CNN might take about 0.4 second to do the job, then it is still a very bad algorithm, since there are 60 frames in one second and to act on real time systems, it shall be able to do atleast 30 operations per second. Though scientists are working consistently on this matter of improving the efficiency of algorithms which can be embedded and shall be working on very low powered devices [1] . Our Approach Here, we will design and talk about those games (a certain class of games which are extremely simple to play) which will can be played automatically, and will see what is the exact power of CNN in solving and automating tasks. There are rather very advanced models like the one which plays Go (alpha go), and others which uses advanced concepts of machine learning. The question was \"Can CNN play pong game?\" . Firstly we have designed and used pong game, designed by Harrison and applied it to our version . The answer lies by asking ourself another question, i.e., can we predict the next move of the player when we are playing one to one with computer by just looking at a static image? as shown in Fig. 1 . Fig. 1: Which direction (up or down?) shall the right paddle move? The answer is NO, we can't predict the move of the Pong by just looking at the image. But, if we are given that the ball is moving towards the player (right paddle), then we may assume that the player might want to go down (just think of an invisible ray coming from the ball and hitting the wall), else, if the ball goes to the direction of the computer, then the player has the tendency to go upward direction (or may be stationary, though it completely depends on how the player is going to act then ). Due to the fact that we are unaware of the direction of the ball, we may sometime move the paddle \"Up\" or sometime move the paddle \"Down\". We might even ignore the cases when the ball is moving away from the player to the computer, since it may be a waste of computation (the AI needs to act only when the ball is coming towards itself). This thing can be sucessfully modelled by using reinforcement learning or some variations of LSTM (Long short term memory model). Our aim is to explore only those subsets of game which are easily and sucessfully played by CNN. Nevertheless, we have made it play, which almost saves the ball from hitting the wall many times, but it doesn't (or rarely) makes score against the Robot computer (since we have tested it with simple CNN model). The video of the performance of the AI is shown in Video 1 . The poor performance of the AI is due to the fact that for making the moves, we need to consider other pictures too (i.e., at least some previous pictures). Video 2: CNN trained pong AI rarely saves the ball from hitting the right wall Tutorial Let's see how we have done this using Python 3. Firstly we need to build a system which can collect the data for the training of the AI. For a CNN to get trained, we need labelled data. For this, we have written a script . The purpose of this script is it takes the action as one hot encoded value (i.e.,a map of number corresponding to the actions), and stores the actions in a CSV (comma separated value) file. The line number corresponds to the frame of the images, and we collect the corresponding image and store it in a different folder. The images obtained are shown in Fig. 2 . Unlike Harrison, we have used the \"human touch\" , by not training it with Robot Pong AI (which calculates the move with the help of certain mathematical constraint and then acts accordingly). The human touch can help the AI do incredeble things, like increasing the speed of the paddle by consistently pressing one button, but in the case of the AI which is trained using the Robot, it would look like the AI is acting optimally everytime by using mathematical constraint, which means it can't do the miracle . In other words, the AI can't take decisions like speeding up when the ball is coming at a high pace towards the paddle, and will be unable to save the ball. By looking at the video we cannot determine whether it is the AI or the human that is playing, since the AI learns from human and this makes any machine learning AI to perform tasks that the human can perform exactly the same way. Script 1: A script in Python3 to create the data for training. Fig. 2: The images collected for training. After the data collection, we need to train it. The Python3 script for training is shown. This is the Cifar-10 model, which has created a benchmark around the year 2015. Script 2: A script in Python3 to train the data. We have trained it on Colab (google collaboratory) using the GPU runtime, by uploading the data in a zip file, unziping it there and downloading the trained .h5py model and testing it on our local machine. The most awesome thing about google collaboratory is that it is free, and uses NVIDIA Tesla T4 GPUs . After downloading the .h5py file, which was trained in a few minutes, we used this script to let the pong AI run in our local machine. Script 3: A script in Python3 to run the AI on local machine. Conclusions When training the AI for 2k images, we get the result as shown in Fig. 3 . This result is pretty amazing, and gives over 85% accuracy on train and test set. Since this is only 2000 images, (for up action = 1K images and for down action= another 1K image) this doesn't fall in the domain of big data. When we are using 30K images, which very minutely falls in the domain of big data (very very minutely), we see how the model is actually affecting the train and test images as shown in Fig. 4 . Fig. 3: The results obtained by training 2k images. Fig. 4: The results obtained by training 30k images. This shows that the game cannot perform good when we are training it with more data. The accuracy of about 56% on the train and 50% on the test data shows that it will not perform well, we need to use a different model. When the data is more, the model have to perform well, if it is not performing well, (when not giving at least 90% accuracy) we need to change our model (without thinking anything else). Your task We have seen that this model doesn't perform well in the case of Pong AI, but we haven't tried for Dino game as shown in Fig. 5 (which is inbuilt in google chrome browser). Your task (should you choose to accept it), shall be to implement this set up for dino game. From intuition we can almost tell the next move by looking at the picture of the dino game, but it may happen that the speed increases over time so, the CNN AI is not able to determine the time for jump when it has not looked the future. For that I have build this game named as jump and roll 2D. This is a dino clone, but with constant speed. The CNN shall give over 90% accuracy for this game, and shall be able to perform satisfactory. The only problem with this game is it is very hard to collect the data for such a booring game like this. But nonetheless, you can use the classic Dino game inbuilt in chrome to make the AI work for only \"day time\". Fig. 5: The famous Dino Game, look ma, no internet! It is bound to give satisfactory result. When done mail me your result! Till then, happy learning! #Update: After working with the data that I collected by playing a hacked version (minimal) of the Google's Dino Game, I got some satisfactory results. The result wasn't perfect because of the lag in my PC to collect the data of the screenshot at the moment when I pressed the \"Jumping action\" (spacebar or uparrow). But, nonetheless, lets see the video of the dino game in action: Video 3: CNN trained dino AI satisfactorily saves itself from hitting and colliding Even from stats perspective we can see the accuracy and error in the following figures. Fig. 6: The accuracy of Dino AI obtained by training 2k images. Fig. 7: The error of Dino AI obtained by training 2k images. Finally our implementation of the model is shown in Fig. 8 . Fig. 8: Our implementation of the CIFAR-10 model. References 1. Jimut Bahan Pal, Shalabh Agarwal, “Real Time Object Detection Can be Embedded on Low Powered Devices”, International Journal of Computer Sciences and Engineering, Vol.7, Issue.2, pp.1005-1009, 2019. 2. The Jupyter Notebook for Colab.",
        "permalink": "https://jimut123.github.io/blogs/cnn_games_ai.html",
        "title": "Playing a class of games using CNN",
        "summary": "Introduction Convolutional Neural Network (CNN) can do a lot of things that were previously consider"
    },
    {
        "content": "Note Please read the updated paper for more detailed insights. JJCluster algorithm is a part of the wisp version - 2 project. Wisp version 2 is now currently having only CLI and can be downloaded from here . Wisp version 0.0.8-b has a GUI and can be downloaded from here . Also, the main page of the project can be found here . ██╗ ██╗██╗███████╗██████╗ ██║ ██║██║██╔════╝██╔══██╗ ██║ █╗ ██║██║███████╗██████╔╝ ██║███╗██║██║╚════██║██╔═══╝ ╚███╔███╔╝██║███████║██║ ╚══╝╚══╝ ╚═╝╚══════╝╚═╝ 2.0s - supporting CLI only JIMUT(TM) Some Visualisations These are some of the vizualisations generated by the clustering algorithm. Kolkata with 8 preference Berlin with 6 preference Tokyo with 15 preference Chicago with 7 preference Delhi with 5 preference Hyderabad with 5 preference Islamabad with 5 preference Jaipur with 5 preference London with 7 preference Moscow with 6 preference Mumbai with 9 preference New York with 7 preference Singapore with 7 preference Srinagar with 4 preference Vadodara with 5 preference Some Demonstrations Video 1: Here is the demo of the application for version - 2 Video 2: Here is the demo of the application for version - 0.0.8-b with a simple GUI. Updated [05-02-2024]: Paper on the same idea published in Nature, which was done by me 5 years ago. They have used multiple baselines for comparison, but essentially the idea and the visualizations looks similar.",
        "permalink": "https://jimut123.github.io/blogs/jjc_wisp.html",
        "title": "Visualizations generated by JJCluster (Jimut-Jisnoo Cluster) algorithm",
        "summary": "Note Please read the updated paper for more detailed insights. JJCluster algorithm is a part of the "
    },
    {
        "content": "These are the games developed by me using Unity3D (please wait patiently while the game loads) and JS. These are mostly compiled to WEBGL framework for HTML. You can play these games in a PC by using web browser online under the same domain. SuperSpartyBrothers Box Shooter 2 Breakout 2D JumpAndRoll2D RollerMadness2 Classic Pong Game in JS-HTML",
        "permalink": "https://jimut123.github.io/blogs/myGames.html",
        "title": "My Games",
        "summary": "These are the games developed by me using Unity3D (please wait patiently while the game loads) and J"
    },
    {
        "content": "What exactly causes dreams? I am referring to Rapid Eye Movement (REM) sleep, not ambitions. There are various types of dreams – dreams that arise from stress (hopefully just a few weeks before exams), dreams that arise from PTSD (Post Traumatic Stress Disorder), lucid dreams, precognitive dreams, dreams that let you lose track of time, haunted dreams, and last but not the least dreams that cause sleeping paralysis. My first memorable dream was probably in class 1, and I was discussing this with my cousins; some had dreams they didn’t remember, and some didn’t even have any dreams. (Source: Pinterest ) Over the last few years, my dreams have become clearer. Not only can I concoct a thrilling story from some of my dreams, but sometimes I feel if I can think of so many inter-connected dependent events in my subconsciousness, what would happen if I was able to use 100% of my cognition? Unfortunately, we only use around 35% of our conscious memory at a given stage . Our mind only remembers stuff that it finds necessary to remember; I have no control over this phenomenon of remembering things at will. I find it very difficult to mug up stuff, and I envy those who can memorize stuff at will. Our way of perceiving things is different. (Source: DALL.E 3) The most common dreams that have been reported across the world are semi-haunted and stressful. I have experienced all sorts of dreams, even precognition. The most common of my dreams has been my failure to reach school on time, but I rarely missed school in real life, maybe two times in a lifetime. The second most common one is I am having an exam, especially some English or Hindi exam, the next day, and I have to read and remember what each of the characters did, along with their name, and probably their credit card numbers too (?) I mean, who does that? This was probably because of the severe punishment (we come from a generation when beating by teachers was legally allowed) for missing unnecessary details of a useless chapter that doesn’t have value, just redundant facts to be memorized and vomited in exams. Even anime have more thrilling stories that give you life lessons. This is simple PTSD. (Source: Pinterest ) The same school gives me lucid memories more often than others. I have a nap, and in two hours, I have lived so much (maybe a full day of any class chosen randomly between 6 to 11). Memories are deceiving; what is concrete are facts. I didn't like school that much until I left it. Those subtle memories of me returning home with my pals, going to coaching, and discussing life on the way home have been etched in my soul. I have some of the best friends from school whom I still cherish to date. I’m not sure why the dream comes in a particular dark-tint color (of evening hue), maybe because I don’t remember much about my past, and things are projected in a different dimension, making these common dreams very lucid. Every time I sleep, I wish I had dreams that kept me in the past when things were so-called simple, my favorite school days. These dreams clear my mind and give me a direction to think positively. So, timelines in dreams can be pretty messed up. Somedays, I sleep only 30 minutes and see content worth 5-6 hours (at least what it feels like), and some days, I sleep for 12 hours and don’t see a dream at all. Though this is rare, I have never had the privilege of sleeping 12 hours continuously, though I may be in bed for that time. (Source: DALL.E 3) Coming to think of haunted dreams, where dreams are particularly eerie, I don’t have much. When I was a kid, I used to see all kinds of ghost movies and had haunted dreams more often, but now I have not watched them (for 15 years?) for good reasons, and I am devoid of such dreams. As time passed by, I often had new ideas come into my dreams, and in most cases, they worked like a charm. As if my subconscious is giving me the solution to a particular problem I have been thinking about for days, if not weeks. Though rare, these kinds of dreams are a blessing in disguise. (Source: Pinterest ) Precognition is a strange phenomenon. I find it intriguing that the human brain can do much more than we see in general. The continuous input to the brain and its simultaneous processing of it makes it a very powerful future visualizer. If a model can see everything that is happening in the world for a very long time, I feel that it can even predict the future to some extent. In my case, and most of the people with whom I have shared this, the dreams take form in real life within a six-month period. I had more than 70 instances where some part of the dream was replicated in the future, giving it a deja-vu-style phenomenon. This is one of the reasons I took a chance in GATE. Even before preparing for the GATE exam, I knew I would be in one of the best institutes in India; though I didn’t know the exact name and location, after coming here and seeing the places, I can relate exactly what I have seen in my dreams. The same goes for RKMVERI and Xavier. This is a really weird phenomenon, making me think I live in a simulation and have been in a loop for a long time. (Source: DALL.E 3) I had my first instance of sleeping paralysis in class 4 when I was on the IIT KGP campus. I still remember I was trying to call my mom since I had a swimming class, but I couldn’t even open my mouth. I felt even a touch could wake me up, but I had to endure till my brain formed a proper connection with all my muscles. Finally, I woke up on time and went to swimming class which was scheduled for 5:30 p.m. This phenomenon didn’t re-appear until I was in class 11, and I had four instances of it in a month. When discussing with people, most said this happens to them too; it feels like someone is holding their body, like a spirit of some sort. I had it over thirty times now; the last two were vivid. I could analyze what exactly is happening. Firstly I thought it was the body that cannot move in the real world, but I was wrong. It happens when your conscious sense wakes up, and you don’t. This generally happens due to reflexes, protecting your body from subtle temperature/pressure changes in your body. Inputs from the real world make the dream form an environment similar to the real setting in which you sleep. Ultimately, you are in a dream, hearing and seeing things in a dream, which is influenced by inputs from the real world. It was believed that sanyasis could feel the environment in the dhyana state, keeping their eyes closed; it is somewhat similar to that. It might be creepy, but it is what it is. The only way to avoid this is to tell yourself that you will wake up; finally, you want to sleep properly for a long time, so sleep peacefully. (Source: Pinterest ) Some people also get dreams which they believe are instances from their past lives which have been etched into their souls. Several Bengali literature terminologies say that these phenomena also occur in real life. I don’t have much experience with them, nor do I get them, though there have been instances where I see a completely different place and later find that these places exist in real life in some way or another. I believe if you think a lot about something, it will get manifested in real life in some way or another. (Source: DALL.E 3) Let us consider a hypothetical situation of sleep with two parallel worlds. The one that you are in, living and doing your job, and the other a similar one, which only activates after you fall asleep in this world and wake up in the other one. Now, time is relative; it doesn’t have to be exact clock hours; even in sleep, you lose track of reality and time. So, which world would you choose if the memories were saved in the dream world? If every time you sleep in this world, it starts right where you ended your dream in that world before entering this one? Wouldn’t you be confused about reality? What if the reality that we live in is a similar phenomenon? Aren’t there too many mysteries in this world that cannot be solved, and we are bound by simulation? We never know…. Edited by Chandan Das.",
        "permalink": "https://jimut123.github.io/blogs/dreams.html",
        "title": "Dreams",
        "summary": "What exactly causes dreams? I am referring to Rapid Eye Movement (REM) sleep, not ambitions. There a"
    },
    {
        "content": "On 16 th October, 2020, I thought of using Pytorch to create a near SOTA classifier for the simplest Computer Vision dataset – MNIST, introduced by Yaan LeCun. Current state of the art is about 99.84% using Capsule networks . I wanted to test out different interesting architectural style for creating internal ensembles of the model, and for that experiment, I used Pytorch! Firstly, let's install the uncommon dependencies, given, you have installed the common ones! ! pip install hiddenlayer graphviz torchviz So, let's dive into the code. Firstly, the basic imports, import numpy as np import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as Data from torchvision import datasets, transforms import torch.nn.functional as F import timeit import unittest We need to add seeds for reproducibility of results in other machines torch.manual_seed( 0 ) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False np.random.seed( 0 ) For running in GPUs, we need to use ‘cuda’, which can be achieved with the following piece of code device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' ) Computer vision datasets can be trained and tested with high accuracy via image augmentations, we have used Cropping, resizing, colour jittering, rotation and random affine transform to make extensive data augmentation. We have also used the mean and standard deviation to normalize the images during training, which will make the deep learning model easier to train. # define a transforms for preparing the dataset transform = transforms.Compose([ transforms.CenterCrop( 26 ), transforms.Resize(( 28 , 28 )), transforms.ColorJitter(brightness= 0.05 , contrast= 0.05 , saturation= 0.05 , hue= 0.05 ), transforms.RandomRotation( 10 ), transforms.RandomAffine( 5 ), # convert the image to a pytorch tensor transforms.ToTensor(), # normalise the images with mean and std of the dataset transforms.Normalize(( 0.1307 ,), ( 0.3081 ,)) ]) After setting the basic stuffs, we will load the dataset into train and test part, which are imported and downloaded from the datasets module. # Load the MNIST training, test datasets using `torchvision.datasets.MNIST` # using the transform defined above train_dataset = datasets.MNIST( './data' ,train= True ,transform=transform,download= True ) test_dataset = datasets.MNIST( './data' ,train= False ,transform=transform,download= True ) This is a relatively small dataset with 60K images, but for large datasets we need to use batch-size, which will store the images in primary memory from the secondary memory before sending it to GPUs. This will create a certain amount of bottleneck in computation, but this is the standard thing to do, when we are dealing with massive amounts of data. # create dataloaders for training and test datasets # use a batch size of 32 and set shuffle=True for the training set train_dataloader = Data.DataLoader(dataset=train_dataset, batch_size= 128 , shuffle= True ) test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size= 128 , shuffle= True ) Now it’s time to build our Deep Neural Network Model! The architecture is some-what shown below. The Pytorch code can be written as: # My Net class Net(nn.Module): def __init__ (self): super(Net, self).__init__() # define a conv layer with output channels as 16, kernel size of 3 and stride of 1 self.conv11 = nn.Conv2d( 1 , 16 , 3 , 1 ) # Input = 1x28x28 Output = 16x26x26 self.conv12 = nn.Conv2d( 1 , 16 , 5 , 1 ) # Input = 1x28x28 Output = 16x24x24 self.conv13 = nn.Conv2d( 1 , 16 , 7 , 1 ) # Input = 1x28x28 Output = 16x22x22 self.conv14 = nn.Conv2d( 1 , 16 , 9 , 1 ) # Input = 1x28x28 Output = 16x20x20 # define a conv layer with output channels as 32, kernel size of 3 and stride of 1 self.conv21 = nn.Conv2d( 16 , 32 , 3 , 1 ) # Input = 16x26x26 Output = 32x24x24 self.conv22 = nn.Conv2d( 16 , 32 , 5 , 1 ) # Input = 16x24x24 Output = 32x20x20 self.conv23 = nn.Conv2d( 16 , 32 , 7 , 1 ) # Input = 16x22x22 Output = 32x16x16 self.conv24 = nn.Conv2d( 16 , 32 , 9 , 1 ) # Input = 16x20x20 Output = 32x12x12 # define a conv layer with output channels as 64, kernel size of 3 and stride of 1 self.conv31 = nn.Conv2d( 32 , 64 , 3 , 1 ) # Input = 32x24x24 Output = 64x22x22 self.conv32 = nn.Conv2d( 32 , 64 , 5 , 1 ) # Input = 32x20x20 Output = 64x16x16 self.conv33 = nn.Conv2d( 32 , 64 , 7 , 1 ) # Input = 32x16x16 Output = 64x10x10 self.conv34 = nn.Conv2d( 32 , 64 , 9 , 1 ) # Input = 32x12x12 Output = 64x4x4 # define a max pooling layer with kernel size 2 self.maxpool = nn.MaxPool2d( 2 ) # Output = 64x11x11 #self.maxpool1 = nn.MaxPool2d(1) # define dropout layer with a probability of 0.25 self.dropout1 = nn.Dropout( 0.25 ) # define dropout layer with a probability of 0.5 self.dropout2 = nn.Dropout( 0.5 ) # define a linear(dense) layer with 128 output features self.fc11 = nn.Linear( 64 * 11 * 11 , 256 ) self.fc12 = nn.Linear( 64 * 8 * 8 , 256 ) # after maxpooling 2x2 self.fc13 = nn.Linear( 64 * 5 * 5 , 256 ) self.fc14 = nn.Linear( 64 * 2 * 2 , 256 ) # define a linear(dense) layer with output features corresponding to the number of classes in the dataset self.fc21 = nn.Linear( 256 , 128 ) self.fc22 = nn.Linear( 256 , 128 ) self.fc23 = nn.Linear( 256 , 128 ) self.fc24 = nn.Linear( 256 , 128 ) self.fc33 = nn.Linear( 128 * 4 , 10 ) #self.fc33 = nn.Linear(64*3,10) def forward (self, inp): # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and # write the forward pass, after each of conv1, conv2, conv3 and fc1 use a relu activation. x = F.relu(self.conv11(inp)) x = F.relu(self.conv21(x)) x = F.relu(self.maxpool(self.conv31(x))) #print(x.shape) #x = torch.flatten(x, 1) x = x.view(- 1 , 64 * 11 * 11 ) x = self.dropout1(x) x = F.relu(self.fc11(x)) x = self.dropout2(x) x = self.fc21(x) y = F.relu(self.conv12(inp)) y = F.relu(self.conv22(y)) y = F.relu(self.maxpool(self.conv32(y))) #x = torch.flatten(x, 1) y = y.view(- 1 , 64 * 8 * 8 ) y = self.dropout1(y) y = F.relu(self.fc12(y)) y = self.dropout2(y) y = self.fc22(y) z = F.relu(self.conv13(inp)) z = F.relu(self.conv23(z)) z = F.relu(self.maxpool(self.conv33(z))) #x = torch.flatten(x, 1) z = z.view(- 1 , 64 * 5 * 5 ) z = self.dropout1(z) z = F.relu(self.fc13(z)) z = self.dropout2(z) z = self.fc23(z) ze = F.relu(self.conv14(inp)) ze = F.relu(self.conv24(ze)) ze = F.relu(self.maxpool(self.conv34(ze))) #x = torch.flatten(x, 1) ze = ze.view(- 1 , 64 * 2 * 2 ) ze = self.dropout1(ze) ze = F.relu(self.fc14(ze)) ze = self.dropout2(ze) ze = self.fc24(ze) out_f = torch.cat((x, y, z, ze), dim= 1 ) #out_f1 = torch.cat((out_f, ze), dim=1) out = self.fc33(out_f) output = F.log_softmax(out, dim= 1 ) return output We can now use the model to set it to GPU. model = Net().to(device) We can even check the parameters: print(model.parameters) Which will result in: <bound method Module.parameters of Net( (conv11): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1)) (conv12): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1)) (conv13): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1)) (conv14): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1)) (conv21): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (conv22): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1)) (conv23): Conv2d(16, 32, kernel_size=(7, 7), stride=(1, 1)) (conv24): Conv2d(16, 32, kernel_size=(9, 9), stride=(1, 1)) (conv31): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)) (conv32): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1)) (conv33): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1)) (conv34): Conv2d(32, 64, kernel_size=(9, 9), stride=(1, 1)) (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (dropout1): Dropout(p=0.25, inplace=False) (dropout2): Dropout(p=0.5, inplace=False) (fc11): Linear(in_features=7744, out_features=256, bias=True) (fc12): Linear(in_features=4096, out_features=256, bias=True) (fc13): Linear(in_features=1600, out_features=256, bias=True) (fc14): Linear(in_features=256, out_features=256, bias=True) (fc21): Linear(in_features=256, out_features=128, bias=True) (fc22): Linear(in_features=256, out_features=128, bias=True) (fc23): Linear(in_features=256, out_features=128, bias=True) (fc24): Linear(in_features=256, out_features=128, bias=True) (fc33): Linear(in_features=512, out_features=10, bias=True) )> It is good to have unit test modules in case of bigger code bases. import unittest class TestImplementations(unittest.TestCase): # Dataloading tests def test_dataset (self): self.dataset_classes = [ '0 - zero' , '1 - one' , '2 - two' , '3 - three' , '4 - four' , '5 - five' , '6 - six' , '7 - seven' , '8 - eight' , '9 - nine' ] self.assertTrue(train_dataset.classes == self.dataset_classes) self.assertTrue(train_dataset.train == True ) def test_dataloader (self): self.assertTrue(train_dataloader.batch_size == 32 ) self.assertTrue(test_dataloader.batch_size == 32 ) def test_total_parameters (self): model = Net().to(device) #self.assertTrue(sum(p.numel() for p in model.parameters()) == 1015946) suite = unittest.TestLoader().loadTestsFromModule(TestImplementations()) unittest.TextTestRunner().run(suite) In the training function, we will pass the data by iterating over the data loaders to model in GPU. We will use the optimizer to calculate the loss, and we will record the losses for plotting in a graph. losses_1 = [] losses_2 = [] def train (model, device, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): # send the image, target to the device data, target = data.to(device), target.to(device) # flush out the gradients stored in optimizer optimizer.zero_grad() # pass the image to the model and assign the output to variable named output output = model(data) # calculate the loss (use nll_loss in pytorch) loss = F.nll_loss(output, target) # do a backward pass loss.backward() # update the weights optimizer.step() if batch_idx % 100 == 0 : print( 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}' .format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) losses_1.append(loss.item()) losses_2.append( 100. * batch_idx / len(train_loader)) Similarly, for test dataset, accuracy = [] avg_loss = [] def test (model, device, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: # send the image, target to the device data, target = data.to(device), target.to(device) # pass the image to the model and assign the output to variable named output output = model(data) test_loss += F.nll_loss(output, target, reduction= 'sum' ).item() # sum up batch loss pred = output.argmax(dim= 1 , keepdim= True ) # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print( '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n' .format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) avg_loss.append(test_loss) accuracy.append( 100. * correct / len(test_loader.dataset)) We can also adjust the learning rate, before firing up for training! model = Net().to(device) learning_rate = [] def adjust_learning_rate (optimizer, iter, each): # sets the learning rate to the initial LR decayed by 0.1 every 'each' iterations lr = 0.001 * ( 0.95 ** (iter // each)) state_dict = optimizer.state_dict() for param_group in state_dict[ 'param_groups' ]: param_group[ 'lr' ] = lr optimizer.load_state_dict(state_dict) print( \"Learning rate = \" ,lr) return lr ## Define Adam Optimiser with a learning rate of 0.01 optimizer = torch.optim.Adam(model.parameters(),lr= 0.001 ) start = timeit.default_timer() for epoch in range( 1 , 100 ): lr = adjust_learning_rate(optimizer, epoch, 1.616 ) learning_rate.append(lr) train(model, device, train_dataloader, optimizer, epoch) test(model, device, test_dataloader) stop = timeit.default_timer() print( 'Total time taken: {} seconds' .format(int(stop - start))) After training for 100 epochs, we get a test accuracy of, 99.57% , which is great, without doing any fancy stuffs! Learning rate = 4.3766309037604346e-05 Train Epoch: 99 [0/60000 (0%)] Loss: 0.000113 Train Epoch: 99 [12800/60000 (21%)] Loss: 0.000007 Train Epoch: 99 [25600/60000 (43%)] Loss: 0.000006 Train Epoch: 99 [38400/60000 (64%)] Loss: 0.000010 Train Epoch: 99 [51200/60000 (85%)] Loss: 0.000027 Test set: Average loss: 0.0211, Accuracy: 9957/10000 (100%) Total time taken: 7074 seconds The images for variation of Learning rate can be shown below: (P.S. look for the typo) Similarly, the average accuracy, average loss, and loss are shown below: The model can be downloaded from here . You can use the model by loading it in Pytorch! It takes lot of innovative methods, to surpass the current state of the art accuracy. If anyone can beat that, it will result in a new paper in a reputed journal. It looks like the model has got into its limit and the test accuracy is fluctuating between 99.57 % and 99.53%. Here is the basic framework for the work, now you are all set up to explore a new domain of competition, via novel methods! Find the notebook [ here ] Still then, Happy Coding!",
        "permalink": "https://jimut123.github.io/blogs/MNIST_rank_17.html",
        "title": "Let's make a MNIST classifier over 99.57% accuracy using Pytorch",
        "summary": "On 16 th October, 2020, I thought of using Pytorch to create a near SOTA classifier for the simplest"
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/IITB_OLD/index.html",
        "title": "Old IIT Bombay Pictures",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/IITB_Courses/IE643/IE643_22D1594_Feed_Forward_Net_Template_Exercise.html",
        "title": "IE 643: Feed Forward Neural Network Demo¶",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/IITB_Courses/IE643/IE643_22D1594_assignment1.html",
        "title": "Question 3 (Assignment 1) IE643",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/IITB_Courses/IE643/IE643_22D1594_solution_2_1_assignment2.html",
        "title": "Question 2(1) (Assignment 2) IE643",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/old_stuffs/modd_img_js.html",
        "title": "MODIFYING AN IMAGE",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/old_stuffs/create_img_js.html",
        "title": "CREATING AN IMAGE FROM SCRATCH",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/old_stuffs/steganography.html",
        "title": "STEGANOGRAPHY",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/old_stuffs/owl_city/owl_city.html",
        "title": "Adam R. Young",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/ML2_Assignment_1_Autoencoders.html",
        "title": "Assignment - 1 (Machine Learning 2)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/LinearRegression-1.html",
        "title": "Assignment - 1 (Machine Learning)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/PolynomialRegressionLASSO-3.html",
        "title": "Assignment - 3 (Machine Learning)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/ML2_Pytorch_Autoencoder.html",
        "title": "With Dropouts¶",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/ColabTricks.html",
        "title": "Colab Tricks",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/LogisticRegression-2.html",
        "title": "Assignment - 2 (Machine Learning)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/kNN-4.html",
        "title": "Assignment - 4 (Machine Learning)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/nnKeras-5.html",
        "title": "Assignment - 5 (ANN)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/ML/CNN_TransferLearning_6.html",
        "title": "Assignment - 6 (Machine Learning)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Hyderabad_5.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Srinagar_4.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Chicago_7.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Berlin_6.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/London_7.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/vadodara_5.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Delhi_5.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Mumbai_9.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Tokyo_15.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Kolkata_8.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Jaipur_5.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Singapore_7.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/New_York_7.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Moscow_6.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/blogs/JJC_WISP/Islamabad_5.html",
        "title": "WISP v2.0JJCluster inside",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/xavo2018/index.html",
        "title": "Xavotsav 2018",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/jimutmap.html",
        "title": "jimutmap",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/mis.html",
        "title": "MIS Application using tkinter",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/super_sparty.html",
        "title": "Super Sparty Brothers 2D",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/wisp.html",
        "title": "Wisp - A preference based location finder application",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/spammify.html",
        "title": "Spammify",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/jimutmap_1.html",
        "title": "jimutmap",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/box_shooter.html",
        "title": "Box Shooter Game",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/captcha.html",
        "title": "Deceiving computers in Reverse Turing Test through Deep Learning",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/analytica_design.html",
        "title": "Graphics design for Analytica 2018 (St. Xavier's College, Mathematics Department)",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/pong.html",
        "title": "Classic Pong Game",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/roller_madness.html",
        "title": "ROLLER MADNESS GAME.",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/memory_game.html",
        "title": "Classic Memory Game",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/analytica.html",
        "title": "Graphics Design - Analytica",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/symmetric.html",
        "title": "Symmetric",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/xavo.html",
        "title": "Xavotsav site 2018",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/jimner.html",
        "title": "jimner - Jimut's banner CLI",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/scrawll.html",
        "title": "Scrawll",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/crawler.html",
        "title": "Pager Rank and Web Crawler",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/jgd.html",
        "title": "JGD- Jimut's Git Downloader",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/stopwatch_game.html",
        "title": "A Stopwatch Game",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/solar_system.html",
        "title": "Solar System Simulation",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/fps.html",
        "title": "FPS GAME beta",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/projects/portfolio.html",
        "title": "Portfolio Page",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/search/index.html",
        "title": "Deep Search Jimut's Website",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/search/index_old.html",
        "title": "Deep Search Jimut's Website",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/vision2024/index.html",
        "title": "CS411: Applications of Computer Vision and Deep Learning",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/vision2025/index.html",
        "title": "Details",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/vision2025/schedule.html",
        "title": "Schedule",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/vision2025/acknowledgements.html",
        "title": "Acknowledgements",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/adsap2024/index.html",
        "title": "Welcome to Our Course Website",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/adsap2024/info.html",
        "title": "Course Structure",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/adsap2024/schedule.html",
        "title": "Course Schedule",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/courses/adsap2024/acknowledge.html",
        "title": "Acknowledgment",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/fonts/icomoon/demo.html",
        "title": "Font Name:icomoon(Glyphs: 1520)",
        "summary": ""
    },
    {
        "content": "After completing end-sem, it was time to do a new adventure! These roads reminds me of IIT KGP We saw some deer at the distant Long walks are my favourite Secluded long roads Sachin looks happy too... just not sure when wild animals might pounce! Trees that have lost leaves My face after reading the sign, be-aware of wild animals. More roads, already completing about 10 kms walk. The true definition of light-green After walking about 15 kms, we finally arrive at Kanheri caves We climed up, there were about 108 caves Mandatory selfie Some antique writings The only drawing that exists in these caves, about 1500 years old. Super tired, and time for return The top of the mountains, and it was time to return hostel!",
        "permalink": "https://jimut123.github.io/trips/trip_sgnp_dec_2022.html",
        "title": "Sanjay Gandhi National Park & Kanheri caves",
        "summary": "After completing end-sem, it was time to do a new adventure! These roads reminds me of IIT KGP We sa"
    },
    {
        "content": "Finally!! after mid sem ended, it was time to go for a trip. A trip that is sponsored by the department. Some of us couldn't sleep the whole night, cause the time at which we needed to be ready, was the time which most of us got to bed. We were ready by 5:30 A.M. and waited for the bus. Boys assembling for the ferry (Mumbai, 6:45 AM). The ferry which took us to Mandwa. Sunrise. Yes we do. After that, we went to Kolaba fort (Alibag) using a speed boat, where we spent some time till 12:00 noon. The obligatory group photo. Greener than kaccha mango bite. CS-726 (Advanced ML) team in a frame, Mota bhai/Deidara/Ninad, Anna/Itachi/Prabhat, Dada/Tobi/Jimut and Bada bhai/Kisame/Sachin. Tank placed near Alibag beach. After lunch at Vasco da Goa. After playing voleyball at Varsoli Beach. Treat from Sona for getting Megan Kacholia fellowship, Thanks Sona! Life at IIT can be extremely stressful, but events like these helps to cope up with the environment!",
        "permalink": "https://jimut123.github.io/trips/cminds_trip_march_2023.html",
        "title": "Departmental trip to Alibag and Varsoli beach",
        "summary": "Finally!! after mid sem ended, it was time to go for a trip. A trip that is sponsored by the departm"
    },
    {
        "content": "It's the month of September, but the rain still continues in Bombay. We set out to the infamous Marine Drive at 9:00 a.m. Great view from cab window. With Prateek in front of the Taj hotel. Here we arrive at the Marine drive! The ML project trio arrives, Sandarbh, Prateek and Jimut Gateway of India They got some great architectures too The classic marine drive seen in movies Obligatory: Eating at Delhi Durbar Somewhat sunset Way more peaceful than the life of an average Ph.D. student Night life at Mumbai is different, after a day's outing, it was time to return hostel.",
        "permalink": "https://jimut123.github.io/trips/trip_marine_drive_sept_2022.html",
        "title": "Marine Drive Trip",
        "summary": "It's the month of September, but the rain still continues in Bombay. We set out to the infamous Mari"
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/trips/abu_dhabi_icip_24.html",
        "title": "ICIP Abu Dhabi Trip - Oct, 2024",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/trips/morocco_miccai_24.html",
        "title": "Morocco and Qatar Trip - Oct, 2024",
        "summary": ""
    },
    {
        "content": "",
        "permalink": "https://jimut123.github.io/demos/symmetric.html",
        "title": "Symmetric",
        "summary": ""
    }
]