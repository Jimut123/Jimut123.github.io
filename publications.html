<!doctype html>


<link href="https://fonts.googleapis.com/css?family=Cardo&display=swap" rel="stylesheet"> 

<link rel="stylesheet" href="../css/bootstrap.min.css"></link>
<!--
<link rel="stylesheet" href="/css/compiled-4.8.0.min.css"></link>
-->
<script type="text/javascript"  src="../js/bootstrap.min.js"></script>

<script type="text/javascript"  src="../js/jquery-3.2.1.slim.min.js"></script>
<script type="text/javascript"  src="../js/popper.min.js"></script>


<link rel='stylesheet' id='wsl-widget-css'  href='https://mdbootstrap.com/wp-content/plugins/wordpress-social-login/assets/css/style.css?ver=5.1.1' type='text/css' media='all' />

<link rel='stylesheet' id='compiled.css-css'  href='https://mdbootstrap.com/wp-content/themes/mdbootstrap4/css/compiled-4.8.0.min.css?ver=4.8.0' type='text/css' media='all' />


<link rel="shortcut icon" type="image/x-icon" href="../img/thumbnail.jpg">


<!-- Bootstrap core CSS -->

<link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">

<!-- Material Design Bootstrap -->

<link href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.8.0/css/mdb.min.css" rel="stylesheet">



<!-- Bootstrap core JavaScript -->

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script>


<!-- Lazy loading of Gallery and stuffs!-->

<script src="../js/jimjs.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css" integrity="sha256-46qynGAkLSFpVbEBog43gvNhfrOj+BmwXdxFgVK/Kvc=" crossorigin="anonymous" /> 

<link rel = "stylesheet" type = "text/css" href = "../css/jimstyle.css" />

<html>
  <head>
    <meta charset="utf-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Jimut Bahan Pal" />
    <meta name="description" content="Personal website">
    <meta name="google-site-verification" content="2SC5t4-rLXOS1eAW-l2YL_qFLuIJW2vSRZNduyOGKN0" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-140291358-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-140291358-1');
    </script>

    
    <title> Publications </title>
  </head>

  <body onload = "myFunction()">
    <div id="loading"> </div>
    
    
<!------ Include the above in your HEAD tag ---------->
<link rel = "stylesheet" type = "text/css" href = "../css/jimstyle.css" />

<nav class="navbar navbar-icon-top navbar-expand-lg navbar-dark bg-dark sticky-top">
  <a class="navbar-brand" href="https://jimut123.github.io/" style="font-family: Georgia, Times, Times New Roman, serif;"> JIMUT </a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" 
  aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">

  <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    
    <ul class="navbar-nav mr-auto navbar-center" style="font-family: Georgia, Times, Times New Roman, serif;">
      
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/"  class="nav-link">
                    
                      
                        <img src="/img/icons/home.svg" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">HOME </font>
                  </a>
                </div>
              </li>
        
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/publications.html"  style="color: #05f270;" class="nav-link">
                    
                      
                        <img src="/img/icons/publications.png" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">PUBLICATIONS </font>
                  </a>
                </div>
              </li>
        
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/blog.html"  class="nav-link">
                    
                      
                        <img src="/img/icons/blog.svg" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">BLOG </font>
                  </a>
                </div>
              </li>
        
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/projects.html"  class="nav-link">
                    
                      
                        <img src="/img/icons/projects.jpg" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">PROJECTS </font>
                  </a>
                </div>
              </li>
        
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/gallery.html"  class="nav-link">
                    
                      
                        <img src="/img/icons/gallery.png" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">GALLERY </font>
                  </a>
                </div>
              </li>
        
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/random.html"  class="nav-link">
                    
                      
                        <img src="/img/icons/certificates.png" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">RANDOM </font>
                  </a>
                </div>
              </li>
        
              <li class="nav-item">
                <div class="container pull-left">
                  <a href="/search/"  class="nav-link">
                    
                      
                        <img src="/img/icons/search.png" alt="icon" height="25px" width="25px">
                      
                   
                    <font size="4px">SEARCH </font>
                  </a>
                </div>
              </li>
        
    </ul>
  
  </div>
</nav>


<script type="text/javascript">
  $(function() {
      // this will get the full URL at the address bar
      var url = window.location.href;

      // passes on every "a" tag
      $(".navbar a").each(function() {
          // checks if its the same on the address bar
          if (url == (this.href)) {
              $(this).closest("li").addClass("active");
              //for making parent of submenu active
             $(this).closest("li").parent().parent().addClass("active");
          }
      });
  });        
</script>


<style>

.navbar ul li.active a, .navbar ul li a:hover {
    background-color: #454747;
    font-weight:bold;
}
</style>



    <div align="center">
    
   
    
      
      <link href="https://fonts.googleapis.com/css?family=Playfair+Display&amp;display=swap" rel="stylesheet" />

<html>
    <h1>
        <b id="heading-font">  PUBLICATIONS</b>
    </h1>
</html>

<!--
    <b><i class="fas fa-chevron-circle-right"></i> <a id = "sucess-link"href="http://ijarcsms.com/docs/paper/volume6/issue10/V6I10-0031.pdf" target="_blank">Botnets: A constant threat to cyberspace </a>. <a id = "sucess-link" onclick="window.open('https://drive.google.com/open?id=196CA1C81ZOd-BzCTQ67anwI_n3ZF1Mnp');
          window.open('https://drive.google.com/open?id=1FWGG9MaOXC169E9g6qwvjTfb4_kRMNgn');" target="_blank">IJARCSMS</a>. Nov 12, 2018. </b>
        <br>
-->
<link href="https://fonts.googleapis.com/css?family=Cardo&amp;display=swap" rel="stylesheet" />

<p><span id="front-page-font">Please refer to the <a href="https://scholar.google.com/citations?user=uyS3AKkAAAAJ&amp;hl=en" target="_blank">Google Scholar</a> page for updated list of publications.</span></p>

<p><br /></p>
<div itemscope="" itemtype="https://schema.org/Person"><a id="front-page-font" itemprop="sameAs" content="https://orcid.org/0000-0002-1206-7902" href="https://orcid.org/0000-0002-1206-7902" target="orcid.widget" rel="noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon" />ORCID ID: 0000-0002-1206-7902</a></div>
<p><br /></p>

<p><!-- Bootstrap Bundle with Popper -->
 <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script></p>

<style>
    .btn:hover
    {
    background-image:none !important;
    background-color:#90e0ef !important;
    }
</style>

<link rel="stylesheet" type="text/css" href="../css/jimstyle.css" />

<div id="pad-container">
    <div class="jumbotron jumbotron-fluid" align="justify">
        <div class="container" id="publication-font">
            

                



                    <div class="row g-0">
                        <div class="col-md-4 text-center my-4 px-2">
                            <img src="../publications/RV_PBS/rv_pbs.jpg" width="100%" style="box-shadow: 4px 4px 8px #888" class="center" />
                        </div>
                        <div class="col-md-8 my-4">
                            <div class="card-body">
                                <h5 class="card-title">
                                    <span id="bold_id">Advancing instance segmentation and WBC classification in peripheral blood smear through domain adaptation - A study on PBC and the novel RV-PBS datasets</span>
                                </h5>
                                <span id="bold_id"> Jimut Bahan Pal</span>, Aniket Bhattacharyea, <strong><a href="http://narendrapur.rkmvu.ac.in/wp-content/uploads/2020/09/Dr.-Debasis-Bandyopadhyay-Banerjee-CV.pdf" target="_blank"> Debasis Banerjee</a></strong> and <strong><a href="https://cs.rkmvu.ac.in/~tamal/" target="_blank" alt="Tamal's RKMVERI Website"> Tamal </a> <a href="http://cs.rkmvu.ac.in/~tamal/" target="_blank" alt="Tamal's RKMVERI Website"> Maharaj </a> </strong>
                                <br />
                                Expert Systems With Applications. (<span id="bold_id">2024</span>). <br /> <a style="color:red;" id="bold_id" href="https://github.com/Jimut123/RV-PBS" target="_blank"> Find the Novel RV-PBS dataset here</a>.
                                <br />
                                <p>

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#abstractrv_pbs" role="button" aria-expanded="false" aria-controls="abstractrv_pbs" id="pub-page-btn-font">Abstract</a>
                                    

                                    
                                        <a type="button" class="btn btn-outline-dark btn-sm" href="https://www.sciencedirect.com/science/article/pii/S0957417424005268" target="_blank" id="pub-page-btn-font">Article</a>
                                    

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="../publications/RV_PBS/thesis_presentation.pdf" target="_blank" id="pub-page-btn-font">Slide</a>
                                    

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="https://github.com/Jimut123/cellseg" target="_blank" id="pub-page-btn-font">Code</a>
                                    

                                   
                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#bibrv_pbs" role="button" aria-expanded="false" aria-controls="bibrv_pbs" id="pub-page-btn-font">Bibtex</a>
                                    
                                   
                                </p>
                                
                                
                                    <div class="collapse" id="abstractrv_pbs">
                                        <div class="card card-body" align="justify">
                                            Automating blood cell counting and detection from smear slides holds significant potential for aiding doctors in disease diagnosis through blood tests. However, existing literature has not adequately addressed using whole slide data in this context. This study introduces the novel RV-PBS dataset, comprising ten distinct peripheral blood smear classes, each featuring multiple multi-class White Blood Cells per slide, specifically designed, for instance segmentation benchmarks. While conventional instance segmentation models like Mask R-CNN exhibit promising results in segmenting medical artifact instances, they face challenges in scenarios with limited samples and class imbalances within the dataset. This challenge prompted us to explore innovative techniques such as domain adaptation using a similar dataset to enhance the classification accuracy of Mask R-CNN, a novel approach in the domain of medical image analysis. Our study has successfully established a comprehensive pipeline capable of segmenting, detecting, and classifying blood samples from slides, striking an optimal balance between computational complexity and accurate classification of medical artifacts. This advancement enables precise cell counting and classification, facilitating doctors in refining their diagnostic analyses.
                                        </div>
                                    </div>
                                

                                
                                    <div class="collapse" id="bibrv_pbs">
                                        <div class="card card-body" align="justify">
                                            <object data="../publications/RV_PBS/pal_2024_rvpbs.txt" width="100%"></object>
                                        </div>
                                    </div>
                                

                                
                                
                            </div>
                        </div>
                    </div>


                



                    <div class="row g-0">
                        <div class="col-md-4 text-center my-4 px-2">
                            <img src="../publications/BMSAN/bmsan.jpg" width="100%" style="box-shadow: 4px 4px 8px #888" class="center" />
                        </div>
                        <div class="col-md-8 my-4">
                            <div class="card-body">
                                <h5 class="card-title">
                                    <span id="bold_id">Improving Multi Scale Attention Networks - Bayesian Optimization for Segmenting medical images</span>
                                </h5>
                                <span id="bold_id"> Jimut Bahan Pal </span> and <strong><a href="https://web.archive.org/web/20200624004219/http://www2.eng.ox.ac.uk/civil/efm/people/dripta-sarkar" target="_blank"> Dripta Mj </a></strong>
                                <br />
                                The Imaging Science Journal. (<span id="bold_id">2023</span>)
                                <br />
                                <p>

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#abstractbmsan" role="button" aria-expanded="false" aria-controls="abstractbmsan" id="pub-page-btn-font">Abstract</a>
                                    

                                    
                                        <a type="button" class="btn btn-outline-dark btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/13682199.2023.2174657" target="_blank" id="pub-page-btn-font">Article</a>
                                    

                                    

                                    

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="https://github.com/Jimut123/bmsan" target="_blank" id="pub-page-btn-font">Code</a>
                                    

                                   
                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#bibbmsan" role="button" aria-expanded="false" aria-controls="bibbmsan" id="pub-page-btn-font">Bibtex</a>
                                    
                                   
                                </p>
                                
                                
                                    <div class="collapse" id="abstractbmsan">
                                        <div class="card card-body" align="justify">
                                            Current deep learning based image segmentation methods are notable for their use of large number of parameters and extensive computational resources in training. There is a persistent need for more efficient flexible systems without compromising on precision. This work proposes a novel model that combines the best of deep learning and probabilistic machine learning to segment a wide variety of medical image datasets with state-of-the-art accuracy and limited resources. The approach benefits from the introduction of new diverse attention modules that serve multiple purposes including capturing of relevant information at different scales. These proposed attention modules are generic and can potentially be used with other architectures to boost performance. In addition, Bayesian optimization is employed to tune multi-scale weight hyperparameters of the model. The architecture combined with one of the proposed novel attention modules and tuned hyperparameters achieves the best results in segmenting ISIC 2017, LUNGS, NERVE, Skin Lesion, and CHEST datasets. Finally, the explainability of the network is analyzed by visualizing the feature map learned from the attention modules.
                                        </div>
                                    </div>
                                

                                
                                    <div class="collapse" id="bibbmsan">
                                        <div class="card card-body" align="justify">
                                            <object data="../publications/BMSAN/bmsan.txt" width="100%"></object>
                                        </div>
                                    </div>
                                

                                
                                
                            </div>
                        </div>
                    </div>


                



                    <div class="row g-0">
                        <div class="col-md-4 text-center my-4 px-2">
                            <img src="../publications/BMIC/bmic.png" width="100%" style="box-shadow: 4px 4px 8px #888" class="center" />
                        </div>
                        <div class="col-md-8 my-4">
                            <div class="card-body">
                                <h5 class="card-title">
                                    <span id="bold_id">Biomedical image analysis competitions - The state of current participation practice</span>
                                </h5>
                                Matthias Eisenmann, Annika Reinke, ... <span id="bold_id"> Jimut Bahan Pal </span> ... Lena Maier-Hein
                                <br />
                                arXiv. (<span id="bold_id">2022</span>)
                                <br />
                                <p>

                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#authors_list_bigbmic" role="button" aria-expanded="false" aria-controls="authors_list_bigbmic" id="pub-page-btn-font">Authors</a>
                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#abstractbmic" role="button" aria-expanded="false" aria-controls="abstractbmic" id="pub-page-btn-font">Abstract</a>
                                    

                                    
                                        <a type="button" class="btn btn-outline-dark btn-sm" href="https://arxiv.org/abs/2212.08568" target="_blank" id="pub-page-btn-font">Article</a>
                                    

                                    

                                    

                                    

                                    

                                   
                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#bibbmic" role="button" aria-expanded="false" aria-controls="bibbmic" id="pub-page-btn-font">Bibtex</a>
                                    
                                   
                                </p>
                                
                                
                                    <div class="collapse" id="abstractbmic">
                                        <div class="card card-body" align="justify">
                                            The number of international benchmarking competitions is steadily increasing in various fields of machine learning (ML) research and practice. This holds especially true for the field of biomedical image analysis, for which dozens of competitions are organized each year. So far, however, little is known about the common practice as well as bottlenecks faced by the community in tackling the research questions posed. To shed light on the status quo of algorithm development for biomedical imaging analysis, we designed an international survey that was issued to all participants of challenges conducted in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) and the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in the year 2021 (n = 80 competitions in total). Besides questions pertaining to general information on the team and the tackled tasks, the survey covered participants’ expertise and working environments, their chosen strategies, as well as algorithm characteristics. A median of 72% challenge participants took part in the survey. According to our results, knowledge exchange was the primary incentive (70%) for participation, while the reception of prize money played only a minor role (16%). While a median of 80 working hours was spent on method development, a large portion of participants stated that they did not have enough time for method development (32%). Surprisingly, only 25% perceived the infrastructure to be a bottleneck. Overall, 94% of all solutions were deep learning-based. Of these, 84% were based on standard architectures. 43% of the respondents reported that the data samples (e.g., images) were too large to be processed at once. This was most commonly addressed by patch-based training (69%), downsampling (37%), and solving 3D analysis tasks as a series of 2D tasks. KSurprisingly, k-fold cross-validation on the training set was performed by only 37% of the participants and only 50% of the participants performed ensembling based on multiple identical models (61%) or heterogeneous models (39%). 48% of the respondents applied postprocessing steps.
                                        </div>
                                    </div>
                                

                                
                                    <div class="collapse" id="bibbmic">
                                        <div class="card card-body" align="justify">
                                            <object data="../publications/BMIC/bmic_arxiv.txt" width="100%"></object>
                                        </div>
                                    </div>
                                

                                
                                    <div class="collapse" id="authors_list_bigbmic">
                                        <div class="card card-body" align="justify">
                                            Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Patrick Godau, Veronika Cheplygina, Michal Kozubek, Sharib Ali, Anubha Gupta, Jan Kybic, Alison Noble, Carlos Ortiz de Solórzano, Samiksha Pachade, Caroline Petitjean, Daniel Sage, Donglai Wei, Elizabeth Wilden, Deepak Alapatt, Vincent Andrearczyk, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Vivek Singh Bawa, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Jinwook Choi, Olivier Commowick, Marie Daum, Adrien Depeursinge, Reuben Dorent, Jan Egger, Hannah Eichhorn, Sandy Engelhardt, Melanie Ganz, Gabriel Girard, Lasse Hansen, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Hyunjeong Kim, Bennett Landman, Hongwei Bran Li, Jianning Li, Jun Ma, Anne Martel, Carlos Martín-Isla, Bjoern Menze, Chinedu Innocent Nwoye, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Carole Sudre, Kimberlin van Wijnen, Armine Vardazaryan, Tom Vercauteren, Martin Wagner, Chuanbo Wang, Moi Hoon Yap, Zeyun Yu, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Rina Bao, Chanyeol Choi, Andrew Cohen, Oleh Dzyubachyk, Adrian Galdran, Tianyuan Gan, Tianqi Guo, Pradyumna Gupta, Mahmood Haithami, Edward Ho, Ikbeom Jang, Zhili Li, Zhengbo Luo, Filip Lux, Sokratis Makrogiannis, Dominik Müller, Young-tack Oh, Subeen Pang, Constantin Pape, Gorkem Polat, Charlotte Rosalie Reed, Kanghyun Ryu, Tim Scherr, Vajira Thambawita, Haoyu Wang, Xinliang Wang, Kele Xu, Hung Yeh, Doyeob Yeo, Yixuan Yuan, Yan Zeng , Xin Zhao, Julian Abbing, Jannes Adam, Nagesh Adluru, Niklas Agethen, Salman Ahmed, Yasmina Al Khalil, Mireia Alenyà, Esa Alhoniemi, Chengyang An, Talha Anwar, Tewodros Weldebirhan Arega, Netanell Avisdris, Dogu Baran Aydogan, Yingbin Bai, Maria Baldeon Calisto, Berke Doga Basaran, Marcel Beetz, Cheng Bian, Hao Bian, Kevin Blansit, Louise Bloch, Robert Bohnsack, Sara Bosticardo, Jack Breen, Mikael Brudfors, Raphael Brüngel, Mariano Cabezas, Alberto Cacciola, Zhiwei Chen, Yucong Chen, Daniel Tianming Chen, Minjeong Cho, Min-Kook Choi, Chuantao Xie Chuantao Xie, Dana Cobzas, Julien Cohen-Adad, Jorge Corral Acero, Sujit Kumar Das, Marcela de Oliveira, Hanqiu Deng, Guiming Dong, Lars Doorenbos, Cory Efird, Di Fan, Mehdi Fatan Serj, Alexandre Fenneteau, Lucas Fidon, Patryk Filipiak, René Finzel, Nuno R. Freitas, Christoph M. Friedrich, Mitchell Fulton, Finn Gaida, Francesco Galati, Christoforos Galazis, Chang Hee Gan, Zheyao Gao, Shengbo Gao, Matej Gazda, Beerend Gerats, Neil Getty, Adam Gibicar, Ryan Gifford, Sajan Gohil, Maria Grammatikopoulou, Daniel Grzech, Orhun Güley, Timo Günnemann, Chunxu Guo, Sylvain Guy, Heonjin Ha, Luyi Han, Il Song Han, Ali Hatamizadeh, Tian He, Jimin Heo, Sebastian Hitziger, SeulGi Hong, SeungBum Hong, Rian Huang, Ziyan Huang, Markus Huellebrand, Stephan Huschauer, Mustaffa Hussain, Tomoo Inubushi, Ece Isik Polat, Mojtaba Jafaritadi, SeongHun Jeong, Bailiang Jian, Yuanhong Jiang, Zhifan Jiang, Yueming Jin, Smriti Joshi, Abdolrahim Kadkhodamohammadi, Reda Abdellah Kamraoui, Inha Kang, Junghwa Kang, Davood Karimi, April Khademi, Muhammad Irfan Khan, Suleiman A. Khan, Rishab Khantwal, Kwang-Ju Kim, Timothy Kline, Satoshi Kondo, Elina Kontio, Adrian Krenzer, Artem Kroviakov, Hugo Kuijf, Satyadwyoom Kumar, Francesco La Rosa, Abhi Lad, Doohee Lee, Minho Lee, Chiara Lena, Hao Li, Ling Li, Xingyu Li, Fuyuan Liao, KuanLun Liao, Arlindo Limede Oliveira, Chaonan Lin, Shan Lin, Akis Linardos, Marius George Linguraru, Han Liu, Tao Liu, Di Liu, Yanling Liu, João Lourenço-Silva, Jingpei Lu, Jiangshan Lu, Imanol Luengo, Christina B. Lund, Huan Minh Luu, Yi Lv, Yi Lv, Uzay Macar, Leon Maechler, Sina Mansour L., Kenji Marshall, Moona Mazher, Richard McKinley, Alfonso Medela, Felix Meissen, Mingyuan Meng, Dylan Miller, Seyed Hossein Mirjahanmardi, Arnab Mishra, Samir Mitha, Hassan Mohy-ud-Din, Tony Chi Wing Mok, Gowtham Krishnan Murugesan, Enamundram Naga Karthik, Sahil Nalawade, Jakub Nalepa, Mohamed Naser, Ramin Nateghi, Hammad Naveed, Quang-Minh Nguyen, Cuong Nguyen Quoc, Brennan Nichyporuk, Bruno Oliveira, David Owen, Jimut Bahan Pal, Junwen Pan, Wentao Pan, Winnie Pang, Bogyu Park, Vivek Pawar, Kamlesh Pawar, Michael Peven, Lena Philipp, Tomasz Pieciak, Szymon Plotka, Marcel Plutat, Fattaneh Pourakpour, Domen Preložnik, Kumaradevan Punithakumar, Abdul Qayyum, Sandro Queirós, Arman Rahmim, Salar Razavi, Jintao Ren, Mina Rezaei, Jonathan Adam Rico, ZunHyan Rieu, Markus Rink, Johannes Roth, Yusely Ruiz-Gonzalez, Numan Saeed, Anindo Saha, Mostafa Salem, Ricardo Sanchez-Matilla, Kurt Schilling, Wei Shao, Zhiqiang Shen, Ruize Shi, Pengcheng Shi, Daniel Sobotka, Théodore Soulier, Bella Specktor Fadida, Danail Stoyanov, Timothy Sum Hon Mun, Xiaowu Sun, Rong Tao, Franz Thaler, Antoine Théberge, Felix Thielke, Helena Torres, Kareem A. Wahid, Jiacheng Wang, YiFei Wang, Wei Wang, Xiong Wang, Jianhui Wen, Ning Wen, Marek Wodzinski, Ye Wu, Fangfang Xia, Tianqi Xiang, Chen Xiaofei, Lizhan Xu, Tingting Xue, Yuxuan Yang, Lin Yang, Kai Yao, Huifeng Yao, Amirsaeed Yazdani, Michael Yip, Hwanseung Yoo, Fereshteh Yousefirizi, Shunkai Yu, Lei Yu, Jonathan Zamora, Ramy Ashraf Zeineldin, Dewen Zeng, Jianpeng Zhang, Bokai Zhang, Jiapeng Zhang, Fan Zhang, Huahong Zhang, Zhongchen Zhao, Zixuan Zhao, Jiachen Zhao, Can Zhao, Qingshuo Zheng, Yuheng Zhi, Ziqi Zhou, Baosheng Zou, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein
                                        </div>
                                    </div>
                                
                                
                            </div>
                        </div>
                    </div>


                



                    <div class="row g-0">
                        <div class="col-md-4 text-center my-4 px-2">
                            <img src="../publications/MICCAI_QUBIQ_21/qubiq_21.jpg" width="100%" style="box-shadow: 4px 4px 8px #888" class="center" />
                        </div>
                        <div class="col-md-8 my-4">
                            <div class="card-body">
                                <h5 class="card-title">
                                    <span id="bold_id">Holistic network for quantifying uncertainties in medical images</span>
                                </h5>
                                <span id="bold_id"> Jimut Bahan Pal </span>
                                <br />
                                International MICCAI Brainlesion Workshop, BrainLes 2021, Brainlesion, Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. (<span id="bold_id">2022</span>)
                                <br />
                                <p>

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#abstractmiccai_qubiq_21" role="button" aria-expanded="false" aria-controls="abstractmiccai_qubiq_21" id="pub-page-btn-font">Abstract</a>
                                    

                                    
                                        <a type="button" class="btn btn-outline-dark btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-09002-8_49" target="_blank" id="pub-page-btn-font">Article</a>
                                    

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="../publications/MICCAI_QUBIQ_21/Jimut_Bahan_Pal_QUBIQ_MICCAI.pdf" target="_blank" id="pub-page-btn-font">Slide</a>
                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="https://www.youtube.com/watch?v=AaVvNG-ihMU" target="_blank" id="pub-page-btn-font">Video</a>
                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="https://github.com/Jimut123/MICCAI_QUBIQ_21" target="_blank" id="pub-page-btn-font">Code</a>
                                    

                                   
                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#bibmiccai_qubiq_21" role="button" aria-expanded="false" aria-controls="bibmiccai_qubiq_21" id="pub-page-btn-font">Bibtex</a>
                                    
                                   
                                </p>
                                
                                
                                    <div class="collapse" id="abstractmiccai_qubiq_21">
                                        <div class="card card-body" align="justify">
                                            Variability in delineation is an inherent property for segmenting medical imagery, when images are annotated by a variety of expert annotators. Previous methods have used adversarial training, Monte-Carlo sampling, and dropouts, which might sometimes produce a wide range of segmentation masks that differ from the styles of mask produced by a set of expert annotators. State-of-the-art method uses multiple U-Nets to capture the individual delineations, but it is computationally demanding. To mitigate this problem, a holistic network containing N-Encoder and N-Decoder is proposed, which could individually model the variability of delineation produced by the expert annotators. This will help to create segmentation masks for different tasks of the same dataset through a single network by learning the common features of multiple Encoders via a common channel and passing those features to Decoder. These create one segmentation mask. All the masks are calculated by using weighted loss at each end of the Decoders that show excellent results for some datasets.
                                        </div>
                                    </div>
                                

                                
                                    <div class="collapse" id="bibmiccai_qubiq_21">
                                        <div class="card card-body" align="justify">
                                            <object data="../publications/MICCAI_QUBIQ_21/miccai_qubiq_21.txt" width="100%"></object>
                                        </div>
                                    </div>
                                

                                
                                
                            </div>
                        </div>
                    </div>


                



                    <div class="row g-0">
                        <div class="col-md-4 text-center my-4 px-2">
                            <img src="../publications/CXR_19/cxr_19.jpg" width="100%" style="box-shadow: 4px 4px 8px #888" class="center" />
                        </div>
                        <div class="col-md-8 my-4">
                            <div class="card-body">
                                <h5 class="card-title">
                                    <span id="bold_id">Classifying Chest X-Ray COVID-19 images via Transfer Learning</span>
                                </h5>
                                <span id="bold_id"> Jimut Bahan Pal </span> and <strong><a href="https://github.com/nlACh" target="_blank"> Nilayan Paul </a></strong>
                                <br />
                                2021 Ethics and Explainability for Responsible Data Science (EE-RDS), published in IEEE Xplore. (<span id="bold_id">2021</span>)
                                <br />
                                <p>

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#abstractcxr_19" role="button" aria-expanded="false" aria-controls="abstractcxr_19" id="pub-page-btn-font">Abstract</a>
                                    

                                    
                                        <a type="button" class="btn btn-outline-dark btn-sm" href="https://ieeexplore.ieee.org/document/9708580" target="_blank" id="pub-page-btn-font">Article</a>
                                    

                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="../publications/CXR_19/CXR_COVID_19_slide.pdf" target="_blank" id="pub-page-btn-font">Slide</a>
                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="https://www.youtube.com/watch?v=27ixHn6SP_4" target="_blank" id="pub-page-btn-font">Video</a>
                                    

                                    
                                        <a class="btn btn-outline-dark btn-sm" href="https://github.com/Jimut123/CXR_Covid-19" target="_blank" id="pub-page-btn-font">Code</a>
                                    

                                   
                                    
                                        <a class="btn btn-outline-dark btn-sm" data-bs-toggle="collapse" href="#bibcxr_19" role="button" aria-expanded="false" aria-controls="bibcxr_19" id="pub-page-btn-font">Bibtex</a>
                                    
                                   
                                </p>
                                
                                
                                    <div class="collapse" id="abstractcxr_19">
                                        <div class="card card-body" align="justify">
                                            The internal behavior of Deep Neural Network architectures can be difficult to interpret. Certain architectures achieve impressive feats in a particular dataset while failing to show comparable performance in other datasets. Developing an architecture that performs well on a dataset can be a time-consuming affair and computationally intensive process. This study explains the effect of transfer learning by fine-tuning already available state-of-the-art architectures in different datasets and using them to classify Chest X-Ray images with high accuracy. Using transfer learning helps the model learn problem-specific features in a short period. It further shows that different models perform differently in a particular setting for a dataset. Ablation studies show that a combination of smaller structures that gives an overall better result may not give the best result in the combined model. In addition, the “belief” of the model for selecting a particular class is visualized in this study.
                                        </div>
                                    </div>
                                

                                
                                    <div class="collapse" id="bibcxr_19">
                                        <div class="card card-body" align="justify">
                                            <object data="../publications/CXR_19/9708580.txt" width="100%"></object>
                                        </div>
                                    </div>
                                

                                
                                
                            </div>
                        </div>
                    </div>


                

 </div>
</div>


<div id="front-page-font">
This material is presented to ensure timely dissemination of scholarly and technical work. 
Copyright and all rights therein are retained by authors or by other copyright holders. 
All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. 
In most cases, these works may not be reposted without the explicit permission of the copyright holder.
</div>

<br />


</div>

      
    
    
  </div>

  


  <!-- Footer -->
  <footer class="footer page-footer font-small elegant-color-dark pt-1">
    <div  align="center">



          <!-- Copyright -->
          <div class="footer-copyright"> 2019-2025
              <a href="https://github.com/Jimut123"> Copyright © Jimut Bahan Pal</a>
          </div>
          <!-- Copyright -->
    </div>
  </footer>
  <!-- Footer -->



</body>
</html>
<style>
.footer{
  position: fixed;
  bottom: 0;
  width: 100%;

}
</style>


<script>
  var preloader = document.getElementById('loading');
  function myFunction() { 
      preloader.style.display = 'none';
  }
</script>